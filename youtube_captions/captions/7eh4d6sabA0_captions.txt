00:00:00 - 00:00:02: [Music]
00:00:01 - 00:00:04: if you're looking for a machine learning
00:00:02 - 00:00:05: tutorial with python and jupyter
00:00:04 - 00:00:07: notebook
00:00:05 - 00:00:09: this tutorial is for you you're going to
00:00:07 - 00:00:11: learn how to solve a real world problem
00:00:09 - 00:00:12: using machine learning and python we're
00:00:11 - 00:00:14: going to start off with a brief
00:00:12 - 00:00:15: introduction to machine learning
00:00:14 - 00:00:17: then we're going to talk about the tools
00:00:15 - 00:00:18: you need and after that we're going to
00:00:17 - 00:00:19: jump straight into the problem we're
00:00:18 - 00:00:21: going to solve
00:00:19 - 00:00:23: you'll learn how to build a model that
00:00:21 - 00:00:24: can learn and predict the kind of music
00:00:23 - 00:00:26: people like
00:00:24 - 00:00:28: so by the end of this one hour tutorial
00:00:26 - 00:00:29: you will have a good understanding of
00:00:28 - 00:00:30: machine learning basics
00:00:29 - 00:00:33: and you'll be able to learn more
00:00:30 - 00:00:34: intermediate to advanced level concepts
00:00:33 - 00:00:35: you don't need any prior knowledge in
00:00:34 - 00:00:37: machine learning but you need to know
00:00:35 - 00:00:38: python fairly well
00:00:37 - 00:00:40: if you don't i've got a couple of
00:00:38 - 00:00:42: tutorials for you here on my channel the
00:00:40 - 00:00:43: links are below this video
00:00:42 - 00:00:45: i'm ashamed only and i'm super excited
00:00:43 - 00:00:47: to be your instructor on this channel i
00:00:45 - 00:00:48: have tons of programming tutorials that
00:00:47 - 00:00:51: you might find helpful
00:00:48 - 00:00:52: so be sure to subscribe as i upload new
00:00:51 - 00:00:59: tutorials every week
00:00:52 - 00:00:59: now let's jump in and get started
00:01:03 - 00:01:06: in this section you're going to learn
00:01:04 - 00:01:07: about machine learning which is a subset
00:01:06 - 00:01:09: of ai or
00:01:07 - 00:01:11: artificial intelligence it's one of the
00:01:09 - 00:01:13: trending topics in the world these days
00:01:11 - 00:01:14: and it's going to have a lot of
00:01:13 - 00:01:16: applications in the future
00:01:14 - 00:01:17: here's an example imagine i ask you to
00:01:16 - 00:01:20: write a program
00:01:17 - 00:01:21: to scan an image and tell if it's a cat
00:01:20 - 00:01:22: or a doc
00:01:21 - 00:01:24: if you want to build this program using
00:01:22 - 00:01:26: traditional programming techniques
00:01:24 - 00:01:28: your program is going to get overly
00:01:26 - 00:01:30: complex you will have to come up with
00:01:28 - 00:01:31: lots of rules to look for specific
00:01:30 - 00:01:33: curves
00:01:31 - 00:01:35: edges and colors in an image to tell if
00:01:33 - 00:01:36: it's a cat or a dog
00:01:35 - 00:01:38: but if i give you a black and white
00:01:36 - 00:01:39: photo your rules may not work
00:01:38 - 00:01:41: they may break then you'll have to
00:01:39 - 00:01:42: rewrite them or i might give you a
00:01:41 - 00:01:44: picture of a cat or a dog
00:01:42 - 00:01:45: from a different angle that you did not
00:01:44 - 00:01:47: predict before
00:01:45 - 00:01:48: so solving this problem using
00:01:47 - 00:01:49: traditional programming techniques is
00:01:48 - 00:01:52: going to get
00:01:49 - 00:01:54: overly complex or sometimes impossible
00:01:52 - 00:01:56: now to make the matter worse what if in
00:01:54 - 00:01:56: the future i ask you to extend this
00:01:56 - 00:01:58: program
00:01:56 - 00:02:00: such that it supports three kinds of
00:01:58 - 00:02:02: animals cats
00:02:00 - 00:02:04: dogs and horses once again you'll have
00:02:02 - 00:02:06: to rewrite all those rules
00:02:04 - 00:02:08: that's not gonna work so machine
00:02:06 - 00:02:09: learning is a technique to solve these
00:02:08 - 00:02:11: kind of problems
00:02:09 - 00:02:12: and this is how it works we build a
00:02:11 - 00:02:15: model or an engine
00:02:12 - 00:02:16: and give it lots and lots of data for
00:02:15 - 00:02:18: example we give you
00:02:16 - 00:02:20: thousands or tens of thousands of
00:02:18 - 00:02:22: pictures of cats and dogs
00:02:20 - 00:02:24: our model will then find and learn
00:02:22 - 00:02:26: patterns in the input data
00:02:24 - 00:02:27: so we can give it a new picture of a cat
00:02:26 - 00:02:29: that it hasn't seen before
00:02:27 - 00:02:31: and ask it is it a cat or a dog or a
00:02:29 - 00:02:32: horse and it will tell us with a certain
00:02:31 - 00:02:34: level of accuracy
00:02:32 - 00:02:36: the more input data we give it the more
00:02:34 - 00:02:38: accurate our model
00:02:36 - 00:02:39: is going to be so that was a very basic
00:02:38 - 00:02:40: example
00:02:39 - 00:02:43: but machine learning has other
00:02:40 - 00:02:45: applications in self-driving cars
00:02:43 - 00:02:46: robotics language processing vision
00:02:45 - 00:02:48: processing
00:02:46 - 00:02:50: forecasting things like stock market
00:02:48 - 00:02:52: trends and the weather
00:02:50 - 00:02:54: games and so on so that's the basic idea
00:02:52 - 00:02:55: about machine learning
00:02:54 - 00:02:58: next we'll look at machine learning in
00:02:55 - 00:02:58: action
00:03:02 - 00:03:06: a machine learning project involves a
00:03:03 - 00:03:08: number of steps the first step is to
00:03:06 - 00:03:09: import our data which often comes in the
00:03:08 - 00:03:11: form of a csv file
00:03:09 - 00:03:13: you might have a database with lots of
00:03:11 - 00:03:15: data we can simply export that data
00:03:13 - 00:03:18: and store it in a csv file for the
00:03:15 - 00:03:20: purpose of our machine learning project
00:03:18 - 00:03:22: so we import our data next we need to
00:03:20 - 00:03:24: clean it and this involves tasks
00:03:22 - 00:03:25: such as removing duplicated data if you
00:03:24 - 00:03:27: have duplicates in the data
00:03:25 - 00:03:29: we don't want to feed this to our model
00:03:27 - 00:03:30: because otherwise our model will learn
00:03:29 - 00:03:32: bad patterns in the data and it will
00:03:30 - 00:03:34: produce the wrong result
00:03:32 - 00:03:36: so we should make sure that our input
00:03:34 - 00:03:37: data is in a good and clean shape
00:03:36 - 00:03:39: if there are data that is irrelevant we
00:03:37 - 00:03:40: should remove them if they are
00:03:39 - 00:03:43: duplicated or
00:03:40 - 00:03:44: incomplete we can remove or modify them
00:03:43 - 00:03:47: if our data is text-based
00:03:44 - 00:03:47: like the name of countries or genres of
00:03:47 - 00:03:50: music
00:03:47 - 00:03:52: or cats and dogs we need to convert them
00:03:50 - 00:03:53: to numerical values
00:03:52 - 00:03:55: so this step really depends on the kind
00:03:53 - 00:03:57: of data we're working with
00:03:55 - 00:03:59: every project is different now that we
00:03:57 - 00:04:00: have a clean data set we need to split
00:03:59 - 00:04:02: it into
00:04:00 - 00:04:04: two segments one for training our model
00:04:02 - 00:04:06: and the other for testing it
00:04:04 - 00:04:08: to make sure that our model produces the
00:04:06 - 00:04:09: right result
00:04:08 - 00:04:11: for example if you have a thousand
00:04:09 - 00:04:12: pictures of cats and dogs
00:04:11 - 00:04:14: we can reserve eighty percent for
00:04:12 - 00:04:17: training and the other 20
00:04:14 - 00:04:18: for testing the next step is to create a
00:04:17 - 00:04:20: model
00:04:18 - 00:04:22: and this involves selecting an algorithm
00:04:20 - 00:04:23: to analyze the data
00:04:22 - 00:04:25: there are so many different machine
00:04:23 - 00:04:26: learning algorithms out there such as
00:04:25 - 00:04:28: decision trees
00:04:26 - 00:04:30: neural networks and so on each algorithm
00:04:28 - 00:04:32: has pros and cons
00:04:30 - 00:04:33: in terms of accuracy and performance so
00:04:32 - 00:04:35: the algorithm you choose
00:04:33 - 00:04:37: depends on the kind of problem you're
00:04:35 - 00:04:39: trying to solve and your input data
00:04:37 - 00:04:41: now the good news is that we don't have
00:04:39 - 00:04:43: to explicitly program an
00:04:41 - 00:04:44: algorithm there are libraries out there
00:04:43 - 00:04:46: that provide these algorithms
00:04:44 - 00:04:48: one of the most popular ones which we
00:04:46 - 00:04:50: are going to look at in this tutorial
00:04:48 - 00:04:52: is scikit-learn so we build a model
00:04:50 - 00:04:54: using an algorithm
00:04:52 - 00:04:56: next we need to train our model so we
00:04:54 - 00:04:58: fitted our training data
00:04:56 - 00:05:00: our model will then look for the
00:04:58 - 00:05:02: patterns in the data so next we can ask
00:05:00 - 00:05:04: it to make predictions
00:05:02 - 00:05:05: back to our example of cats and dogs we
00:05:04 - 00:05:07: can ask our model
00:05:05 - 00:05:09: is this a cat or a dog and our model
00:05:07 - 00:05:11: will make a prediction
00:05:09 - 00:05:13: now the prediction is not always
00:05:11 - 00:05:14: accurate in fact when you start out
00:05:13 - 00:05:17: it's very likely that your predictions
00:05:14 - 00:05:18: are inaccurate so we need to evaluate
00:05:17 - 00:05:20: the predictions and measure
00:05:18 - 00:05:21: their accuracy then we need to get back
00:05:20 - 00:05:24: to our model
00:05:21 - 00:05:25: and either select a different algorithm
00:05:24 - 00:05:27: that is going to produce
00:05:25 - 00:05:28: a more accurate result for the kind of
00:05:27 - 00:05:32: problem we're trying to solve
00:05:28 - 00:05:34: or fine-tune the parameters of our model
00:05:32 - 00:05:37: so each algorithm has parameters that we
00:05:34 - 00:05:38: can modify to optimize the accuracy
00:05:37 - 00:05:41: so these are the high level steps that
00:05:38 - 00:05:42: you follow in a machine learning project
00:05:41 - 00:05:45: next we'll look at the libraries and
00:05:42 - 00:05:45: tools for machine learning
00:05:50 - 00:05:53: in this lecture we're going to look at
00:05:51 - 00:05:55: the popular python libraries that we use
00:05:53 - 00:05:57: in machine learning projects
00:05:55 - 00:05:58: the first one is numpy which provides a
00:05:57 - 00:06:01: multi-dimensional array
00:05:58 - 00:06:03: very very popular library the second one
00:06:01 - 00:06:05: is pandas which is a data analysis
00:06:03 - 00:06:06: library that provides a concept called
00:06:05 - 00:06:08: data frame
00:06:06 - 00:06:10: a data frame is a two-dimensional data
00:06:08 - 00:06:11: structure similar to an excel
00:06:10 - 00:06:13: spreadsheet
00:06:11 - 00:06:15: so we have rows and columns we can
00:06:13 - 00:06:18: select data in a row or a column
00:06:15 - 00:06:20: or a range of rows and columns again
00:06:18 - 00:06:22: very very popular in machine learning
00:06:20 - 00:06:23: and data science projects the third
00:06:22 - 00:06:25: library
00:06:23 - 00:06:26: is matplotlib which is a two-dimensional
00:06:25 - 00:06:29: plotting library
00:06:26 - 00:06:31: for creating graphs and plots the next
00:06:29 - 00:06:32: library is scikit-learn which is one of
00:06:31 - 00:06:33: the most popular machine learning
00:06:32 - 00:06:35: libraries
00:06:33 - 00:06:37: that provides all these common
00:06:35 - 00:06:40: algorithms like decision trees
00:06:37 - 00:06:41: neural networks and so on now when
00:06:40 - 00:06:43: working with machine learning projects
00:06:41 - 00:06:45: we use an environment called jupiter for
00:06:43 - 00:06:46: writing our code
00:06:45 - 00:06:48: technically we can still use vs code or
00:06:46 - 00:06:49: any other code editors
00:06:48 - 00:06:51: but these editors are not ideal for
00:06:49 - 00:06:52: machine learning projects
00:06:51 - 00:06:54: because we frequently need to inspect
00:06:52 - 00:06:56: the data and that is really hard
00:06:54 - 00:06:58: in environments like vs code and
00:06:56 - 00:07:00: terminal if you're working with a table
00:06:58 - 00:07:01: of 10 or 20 columns
00:07:00 - 00:07:03: visualizing this data in a terminal
00:07:01 - 00:07:04: window is really really difficult and
00:07:03 - 00:07:06: messy
00:07:04 - 00:07:08: so that's why we use jupiter it makes it
00:07:06 - 00:07:10: really easy to inspect our data
00:07:08 - 00:07:13: now to install jupyter we're going to
00:07:10 - 00:07:16: use a platform called anaconda
00:07:13 - 00:07:18: so head over to anaconda.com
00:07:16 - 00:07:20: download on this page you can download
00:07:18 - 00:07:21: anaconda distribution for your operating
00:07:20 - 00:07:24: system
00:07:21 - 00:07:28: so we have distributions for windows mac
00:07:24 - 00:07:32: and linux so let's go ahead and
00:07:28 - 00:07:34: install anaconda for python 3.7
00:07:32 - 00:07:34: download
00:07:36 - 00:07:41: all right so here's anaconda downloaded
00:07:38 - 00:07:42: on my machine let's double click this
00:07:41 - 00:07:44: all right first it's going to run a
00:07:42 - 00:07:45: program to determine if the software can
00:07:44 - 00:07:48: be installed
00:07:45 - 00:07:49: so let's continue and once again
00:07:48 - 00:07:51: continue
00:07:49 - 00:07:52: continue pretty easy continue one more
00:07:51 - 00:07:55: time
00:07:52 - 00:07:57: i agree with the license agreement okay
00:07:55 - 00:07:59: you can use the default installation
00:07:57 - 00:08:02: location so don't worry about that
00:07:59 - 00:08:04: just click install give it a few seconds
00:08:02 - 00:08:06: now the beautiful thing about anaconda
00:08:04 - 00:08:08: is that it will install jupyter
00:08:06 - 00:08:10: as well as all those popular data
00:08:08 - 00:08:11: science libraries like numpy
00:08:10 - 00:08:15: pandas and so on so we don't have to
00:08:11 - 00:08:15: manually install this using pip
00:08:16 - 00:08:19: all right now as part of the next step
00:08:17 - 00:08:21: anaconda is suggesting to install
00:08:19 - 00:08:22: microsoft vs code we already have this
00:08:21 - 00:08:24: on our machine so
00:08:22 - 00:08:26: we don't have to install it we can go
00:08:24 - 00:08:28: with continue and
00:08:26 - 00:08:30: close the installation now finally we
00:08:28 - 00:08:33: can move this to trash because we don't
00:08:30 - 00:08:35: need this installer in the future
00:08:33 - 00:08:36: all right now open up a terminal window
00:08:35 - 00:08:40: and type
00:08:36 - 00:08:43: jupyter with a y space
00:08:40 - 00:08:44: notebook this will start the notebook
00:08:43 - 00:08:48: server on your machine
00:08:44 - 00:08:49: so enter there you go this will start
00:08:48 - 00:08:51: the notebook server on your machine you
00:08:49 - 00:08:53: can see these default messages here
00:08:51 - 00:08:55: don't worry about them now it
00:08:53 - 00:09:00: automatically opens a browser window
00:08:55 - 00:09:02: pointing to localhost port 888
00:09:00 - 00:09:04: this is what we call jupiter dashboard
00:09:02 - 00:09:05: on this dashboard we have a few tabs the
00:09:04 - 00:09:07: first tab
00:09:05 - 00:09:09: is the files tab and by default this
00:09:07 - 00:09:11: points to your home directory
00:09:09 - 00:09:13: so every user on your machine has a home
00:09:11 - 00:09:13: directory this is my home directory on
00:09:13 - 00:09:15: mac
00:09:13 - 00:09:17: you can see here we have a desktop
00:09:15 - 00:09:18: folder as well as documents
00:09:17 - 00:09:20: downloads and so on on your machine
00:09:18 - 00:09:21: you're going to see different folders
00:09:20 - 00:09:23: so someone on your machine you need to
00:09:21 - 00:09:26: create a jupyter notebook
00:09:23 - 00:09:28: i'm going to go to desktop here's my
00:09:26 - 00:09:31: desktop i don't have anything here
00:09:28 - 00:09:32: and then click new i want to create a
00:09:31 - 00:09:34: notebook
00:09:32 - 00:09:35: for python 3. in this notebook we can
00:09:34 - 00:09:37: write python code
00:09:35 - 00:09:39: and execute it line by line we can
00:09:37 - 00:09:41: easily visualize our data as you will
00:09:39 - 00:09:44: see over the next few videos
00:09:41 - 00:09:44: so let's go ahead with this
00:09:45 - 00:09:48: all right here's our first notebook you
00:09:47 - 00:09:52: can see by default it's called
00:09:48 - 00:09:52: untitled let's change that to hello
00:09:52 - 00:09:54: world
00:09:52 - 00:09:56: so this is going to be the hello world
00:09:54 - 00:09:59: of our machine learning project
00:09:56 - 00:10:01: let's rename this now if you look at
00:09:59 - 00:10:02: your desktop you can see this file
00:10:01 - 00:10:06: helloworld.i
00:10:02 - 00:10:08: pi nb this is a jupiter notebook
00:10:06 - 00:10:09: it's kind of similar to our pi files
00:10:08 - 00:10:11: where we write our python code
00:10:09 - 00:10:15: but it includes additional data that
00:10:11 - 00:10:18: jupiter uses to execute our code
00:10:15 - 00:10:22: so back to our notebook let's do a
00:10:18 - 00:10:25: print hello world and then
00:10:22 - 00:10:28: click this run button here and
00:10:25 - 00:10:29: here's the result printed in jupyter so
00:10:28 - 00:10:31: we don't have to navigate back and forth
00:10:29 - 00:10:34: between the terminal window
00:10:31 - 00:10:36: we can see all the result right here
00:10:34 - 00:10:40: next i'm going to show you how to load a
00:10:36 - 00:10:40: data set from a csv file in jupyter
00:10:44 - 00:10:48: all right in this lecture we're going to
00:10:46 - 00:10:49: download a data set from a very popular
00:10:48 - 00:10:52: website called
00:10:49 - 00:10:54: kaggle.com gaggle is basically a place
00:10:52 - 00:10:55: to do data science projects
00:10:54 - 00:10:57: so the first thing you need to do is to
00:10:55 - 00:10:59: create an account you can sign up with
00:10:57 - 00:11:00: facebook google or using a custom email
00:10:59 - 00:11:02: and password
00:11:00 - 00:11:04: once you sign up then come back here on
00:11:02 - 00:11:07: kaggle.com
00:11:04 - 00:11:10: here in the search bar search for
00:11:07 - 00:11:12: video game sales this is the name of a
00:11:10 - 00:11:13: very popular data set that we're going
00:11:12 - 00:11:15: to use in this lecture
00:11:13 - 00:11:16: so here in this list you can see the
00:11:15 - 00:11:19: first item
00:11:16 - 00:11:21: with this kind of reddish icon so let's
00:11:19 - 00:11:22: go with that
00:11:21 - 00:11:25: as you can see this data set includes
00:11:22 - 00:11:27: the sales data for more than 16
00:11:25 - 00:11:28: 000 video games on this page you can see
00:11:27 - 00:11:30: the description
00:11:28 - 00:11:32: of various columns in this data set we
00:11:30 - 00:11:35: have rank
00:11:32 - 00:11:38: name platform year and so on so here's
00:11:35 - 00:11:39: our data source it's a csv file called
00:11:38 - 00:11:42: vgsales.csv
00:11:39 - 00:11:43: as you can see there are over 16 000
00:11:42 - 00:11:46: rows
00:11:43 - 00:11:47: and 11 columns in this data set right
00:11:46 - 00:11:48: below that you can see the first few
00:11:47 - 00:11:50: records
00:11:48 - 00:11:53: of this data set so here's our first
00:11:50 - 00:11:55: record the ranking for this game is one
00:11:53 - 00:11:56: it's the wii sports game for we as the
00:11:55 - 00:11:58: platform and it was released in year
00:11:56 - 00:12:01: 2006
00:11:58 - 00:12:01: now what i want you to do is to go ahead
00:12:01 - 00:12:04: and
00:12:01 - 00:12:05: download this data set and as i told you
00:12:04 - 00:12:07: before you need to sign in before you
00:12:05 - 00:12:09: can download this
00:12:07 - 00:12:10: so this will give you a zip file as you
00:12:09 - 00:12:13: can see here
00:12:10 - 00:12:14: here's our csv file now i want you to
00:12:13 - 00:12:16: put this
00:12:14 - 00:12:18: right next to your jupyter notebook on
00:12:16 - 00:12:20: my machine that is on my desktop
00:12:18 - 00:12:22: so i'm going to drag and drop this onto
00:12:20 - 00:12:24: the desktop folder
00:12:22 - 00:12:25: now if you look at the desktop you can
00:12:24 - 00:12:28: see here is my
00:12:25 - 00:12:30: jupyter hello world notebook and right
00:12:28 - 00:12:33: next to that we have
00:12:30 - 00:12:35: vgsales.csv with that
00:12:33 - 00:12:36: we go back to our jupyter notebook let's
00:12:35 - 00:12:40: remove the first line
00:12:36 - 00:12:43: and instead import
00:12:40 - 00:12:45: pandas as pd
00:12:43 - 00:12:46: with this we're importing pandas module
00:12:45 - 00:12:49: and renaming it to pd
00:12:46 - 00:12:51: so we don't have to type pandas dot
00:12:49 - 00:12:54: several times in this code
00:12:51 - 00:12:58: now let's type pd dot read
00:12:54 - 00:12:59: underline csv and pass the name of our
00:12:58 - 00:13:03: csv file
00:12:59 - 00:13:04: that is vg sales.csv now because this
00:13:03 - 00:13:06: csv file is in the current folder
00:13:04 - 00:13:08: right next to our jupyter notebook we
00:13:06 - 00:13:11: can easily load it otherwise we'll have
00:13:08 - 00:13:14: to supply the full path to this file
00:13:11 - 00:13:17: so this returns a data frame object
00:13:14 - 00:13:18: which is like an excel spreadsheet let
00:13:17 - 00:13:21: me show you so
00:13:18 - 00:13:21: we store it here and then we can simply
00:13:21 - 00:13:24: type
00:13:21 - 00:13:26: df to inspect it so one more time let's
00:13:24 - 00:13:28: run this program
00:13:26 - 00:13:30: here's our data frame with these rows
00:13:28 - 00:13:33: and columns so we have rank
00:13:30 - 00:13:35: name platform and so on now this data
00:13:33 - 00:13:35: frame object has lots of attributes and
00:13:35 - 00:13:37: methods
00:13:35 - 00:13:38: that we're not going to cover in this
00:13:37 - 00:13:39: tutorial that's really beyond the scope
00:13:38 - 00:13:41: of what we're going to do
00:13:39 - 00:13:43: so i'll leave it up to you to read
00:13:41 - 00:13:45: panda's documentation or follow other
00:13:43 - 00:13:46: tutorials to learn about pandas data
00:13:45 - 00:13:47: frames
00:13:46 - 00:13:49: but in this lecture i'm going to show
00:13:47 - 00:13:52: you some of the most useful methods
00:13:49 - 00:13:55: and attributes the first one is shape so
00:13:52 - 00:13:56: shape let's run this one more time so
00:13:55 - 00:14:00: here's the shape of this data set
00:13:56 - 00:14:03: we have over 16 000 records and
00:14:00 - 00:14:04: 11 columns technically this is a two
00:14:03 - 00:14:07: dimensional array
00:14:04 - 00:14:10: of sixteen thousand and eleven okay
00:14:07 - 00:14:11: now you can see here we have another
00:14:10 - 00:14:13: segment for writing code so we don't
00:14:11 - 00:14:14: have to write all the code in the first
00:14:13 - 00:14:15: segment
00:14:14 - 00:14:17: so here in the second segment we can
00:14:15 - 00:14:17: call one of the methods of the data
00:14:17 - 00:14:22: frame
00:14:17 - 00:14:25: that is df dot describe
00:14:22 - 00:14:27: now when we run this program we can see
00:14:25 - 00:14:29: the output for each segment
00:14:27 - 00:14:30: right next to it so here's our first
00:14:29 - 00:14:33: segment here we have
00:14:30 - 00:14:36: these three lines and this is the output
00:14:33 - 00:14:37: of the last line below that we have our
00:14:36 - 00:14:38: second segment
00:14:37 - 00:14:40: here we're calling the describe method
00:14:38 - 00:14:43: and right below that we have
00:14:40 - 00:14:44: the output of this segment so this is
00:14:43 - 00:14:46: the beauty of jupiter
00:14:44 - 00:14:48: we can easily visualize our data doing
00:14:46 - 00:14:50: this with vs code and terminal windows
00:14:48 - 00:14:52: is really tedious and clunky
00:14:50 - 00:14:54: so what is this describe method
00:14:52 - 00:14:56: returning basically it's returning some
00:14:54 - 00:14:58: basic information about each column
00:14:56 - 00:15:00: in this data set so as you saw earlier
00:14:58 - 00:15:02: we have columns like rank
00:15:00 - 00:15:04: year and so on these are the columns
00:15:02 - 00:15:07: with numerical values
00:15:04 - 00:15:08: now for each column we have the count
00:15:07 - 00:15:09: which is the number of records in that
00:15:08 - 00:15:12: column
00:15:09 - 00:15:16: you can see our rank column has 16
00:15:12 - 00:15:19: 598 records whereas the year column has
00:15:16 - 00:15:21: 16 327 records
00:15:19 - 00:15:24: so this shows that some of our records
00:15:21 - 00:15:26: don't have the value for the year column
00:15:24 - 00:15:27: we have no values so in a real data
00:15:26 - 00:15:28: science or
00:15:27 - 00:15:30: machine learning project we'll have to
00:15:28 - 00:15:31: use some techniques to clean up our data
00:15:30 - 00:15:33: set
00:15:31 - 00:15:35: one option is to remove the records that
00:15:33 - 00:15:38: don't have a value for the year column
00:15:35 - 00:15:40: or we can assign them a default value
00:15:38 - 00:15:42: that really depends on the project
00:15:40 - 00:15:44: now another attribute for each column is
00:15:42 - 00:15:46: mean so this is the average
00:15:44 - 00:15:48: of all the values now in the case of the
00:15:46 - 00:15:48: rank column this value doesn't really
00:15:48 - 00:15:51: matter
00:15:48 - 00:15:53: but look at the year so the average year
00:15:51 - 00:15:53: for all these video games in our data
00:15:53 - 00:15:56: set
00:15:53 - 00:15:57: is 2006 and this might be important in
00:15:56 - 00:16:00: the problem we're trying to solve
00:15:57 - 00:16:01: we also have standard deviation which is
00:16:00 - 00:16:03: a measure
00:16:01 - 00:16:05: to quantify the amount of variation in
00:16:03 - 00:16:08: our set of values
00:16:05 - 00:16:10: below that we have min as an example the
00:16:08 - 00:16:13: minimum value for the year column
00:16:10 - 00:16:14: is 1980. so quite often when we work
00:16:13 - 00:16:16: with a new data set
00:16:14 - 00:16:18: we call the describe method to get some
00:16:16 - 00:16:21: basic statistics about our data
00:16:18 - 00:16:24: let me show you another useful attribute
00:16:21 - 00:16:28: so in the next segment let's type
00:16:24 - 00:16:29: df.values let's run this
00:16:28 - 00:16:31: as you can see this returns a
00:16:29 - 00:16:32: two-dimensional array this square
00:16:31 - 00:16:36: bracket indicates
00:16:32 - 00:16:38: the outer array and the second one
00:16:36 - 00:16:39: represents the inner array so the first
00:16:38 - 00:16:42: element
00:16:39 - 00:16:45: in our outer array is an array itself
00:16:42 - 00:16:47: these are the values in this array which
00:16:45 - 00:16:49: basically represent the first row
00:16:47 - 00:16:50: in our data set so the video game with
00:16:49 - 00:16:53: ranking 1
00:16:50 - 00:16:53: which is called wii sports so this was a
00:16:53 - 00:16:56: basic
00:16:53 - 00:16:57: overview of pando's data frames in the
00:16:56 - 00:17:05: next lecture i'm going to show you some
00:16:57 - 00:17:05: of the useful shortcuts of jupyter
00:17:06 - 00:17:09: in this lecture i'm going to show you
00:17:07 - 00:17:10: some of the most useful shortcuts in
00:17:09 - 00:17:11: jupyter
00:17:10 - 00:17:13: now the first thing i want you to pay
00:17:11 - 00:17:16: attention to is this green bar
00:17:13 - 00:17:18: on the left this indicates that this
00:17:16 - 00:17:21: cell is currently in the edit mode so
00:17:18 - 00:17:25: we can write code here now if we press
00:17:21 - 00:17:27: the escape key green turns to blue and
00:17:25 - 00:17:28: that means this cell is currently in the
00:17:27 - 00:17:30: command mode
00:17:28 - 00:17:32: so basically the activated cell can be
00:17:30 - 00:17:34: either in the edit mode or
00:17:32 - 00:17:36: the command mode depending on the mode
00:17:34 - 00:17:38: we have different shortcuts
00:17:36 - 00:17:40: so here we're currently in the command
00:17:38 - 00:17:42: mode if we press
00:17:40 - 00:17:43: h we can see the list of all the
00:17:42 - 00:17:46: keyboard
00:17:43 - 00:17:48: shortcuts right above this list you can
00:17:46 - 00:17:51: see
00:17:48 - 00:17:53: mac os modifier keys these are the extra
00:17:51 - 00:17:54: keys that we have on a mac keyboard
00:17:53 - 00:17:57: if you're a windows user you're not
00:17:54 - 00:18:00: going to see this so as an example here
00:17:57 - 00:18:01: is the shape of the command key this is
00:18:00 - 00:18:04: control this is
00:18:01 - 00:18:06: option and so on with this guideline you
00:18:04 - 00:18:07: can easily understand the shortcut
00:18:06 - 00:18:10: associated with each command
00:18:07 - 00:18:11: let me show you so here we have all the
00:18:10 - 00:18:13: commands
00:18:11 - 00:18:15: when a cell is in the command mode for
00:18:13 - 00:18:18: example we have this command
00:18:15 - 00:18:20: open the command palette this is exactly
00:18:18 - 00:18:21: like the command palette that we have in
00:18:20 - 00:18:23: vs code
00:18:21 - 00:18:24: here's a shortcut to execute this
00:18:23 - 00:18:29: command that is
00:18:24 - 00:18:31: command shift and f okay so here we have
00:18:29 - 00:18:32: lots of shortcuts of course you're not
00:18:31 - 00:18:34: going to use all of them all the time
00:18:32 - 00:18:36: but it's good to have a quick look here
00:18:34 - 00:18:37: to see what is available for you
00:18:36 - 00:18:39: with this shortcuts you can write code
00:18:37 - 00:18:41: much faster so let me show you some of
00:18:39 - 00:18:44: the most useful ones
00:18:41 - 00:18:45: i'm going to close this now with our
00:18:44 - 00:18:49: first cell
00:18:45 - 00:18:52: in the command mode i'm going to press b
00:18:49 - 00:18:53: and this inserts a new cell below this
00:18:52 - 00:18:55: cell
00:18:53 - 00:18:57: we can also go back to our first cell
00:18:55 - 00:18:59: press escape
00:18:57 - 00:19:00: now the cell is in the command mode we
00:18:59 - 00:19:03: can insert an empty cell
00:19:00 - 00:19:06: above this cell by pressing a
00:19:03 - 00:19:09: so either a or b a for above and b
00:19:06 - 00:19:10: for below okay now if you don't want
00:19:09 - 00:19:14: this cell you can press
00:19:10 - 00:19:15: d twice to delete it like this
00:19:14 - 00:19:18: now in the cell i'm going to print a
00:19:15 - 00:19:22: hello world message so
00:19:18 - 00:19:24: print hello world now
00:19:22 - 00:19:27: to run the code in this cell we can
00:19:24 - 00:19:30: click on the run button here
00:19:27 - 00:19:31: so here's our print function and right
00:19:30 - 00:19:34: below that you can see
00:19:31 - 00:19:35: the output of this function but note
00:19:34 - 00:19:38: that when you run a cell
00:19:35 - 00:19:38: this will only execute the code in that
00:19:38 - 00:19:41: cell
00:19:38 - 00:19:43: in other words the code in other cells
00:19:41 - 00:19:44: will not be executed let me show you
00:19:43 - 00:19:46: what i mean
00:19:44 - 00:19:47: so in the cell below this cell i'm going
00:19:46 - 00:19:50: to delete the call
00:19:47 - 00:19:51: to describe method instead i'm going to
00:19:50 - 00:19:55: print
00:19:51 - 00:19:57: ocean now i'm going to put the cursor
00:19:55 - 00:19:59: back in this cell where we print the
00:19:57 - 00:20:02: hello world message
00:19:59 - 00:20:02: and run this cell so you can see hello
00:20:02 - 00:20:05: world
00:20:02 - 00:20:06: is displayed here but the cell below is
00:20:05 - 00:20:09: still displaying
00:20:06 - 00:20:10: the described table so we don't see the
00:20:09 - 00:20:13: changes here
00:20:10 - 00:20:14: now to solve this problem we can go to
00:20:13 - 00:20:18: the cell menu on the top
00:20:14 - 00:20:20: and run all cells together
00:20:18 - 00:20:21: this can work for small projects but
00:20:20 - 00:20:22: sometimes you're working with a large
00:20:21 - 00:20:24: data set
00:20:22 - 00:20:25: so if you want to run all these cells
00:20:24 - 00:20:26: together it's going to take a lot of
00:20:25 - 00:20:28: time
00:20:26 - 00:20:30: that is the reason jupiter saves the
00:20:28 - 00:20:31: output of itself so we don't have to
00:20:30 - 00:20:34: rerun that code
00:20:31 - 00:20:35: if it hasn't changed so this notebook
00:20:34 - 00:20:38: file that we have here
00:20:35 - 00:20:38: includes our source code organized in
00:20:38 - 00:20:40: cells
00:20:38 - 00:20:42: as well as the output for each cell that
00:20:40 - 00:20:44: is why it's different
00:20:42 - 00:20:46: from a regular pi file where we only
00:20:44 - 00:20:48: have the source code
00:20:46 - 00:20:49: here we also have autocompletion and
00:20:48 - 00:20:52: intellisense
00:20:49 - 00:20:56: so in the cell let's call
00:20:52 - 00:20:58: df dataframe dot
00:20:56 - 00:21:00: now if you press tab we can see all the
00:20:58 - 00:21:03: attributes and methods
00:21:00 - 00:21:05: in this object so let's call
00:21:03 - 00:21:07: describe now with the cursor on the name
00:21:05 - 00:21:10: of the method we can press
00:21:07 - 00:21:11: shift and tab to see this tooltip that
00:21:10 - 00:21:13: describes
00:21:11 - 00:21:15: what this method does and what parameter
00:21:13 - 00:21:16: it takes so here in front of signature
00:21:15 - 00:21:18: you can see
00:21:16 - 00:21:19: the describe method these are the
00:21:18 - 00:21:21: parameters
00:21:19 - 00:21:22: and their default value and right below
00:21:21 - 00:21:25: that you can see
00:21:22 - 00:21:26: the description of what this method does
00:21:25 - 00:21:28: in this case it generates
00:21:26 - 00:21:31: descriptive statistics that summarize
00:21:28 - 00:21:33: the central tendency and so on
00:21:31 - 00:21:34: similar to vs code we can also convert a
00:21:33 - 00:21:37: line to comment
00:21:34 - 00:21:40: by pressing command and slash on mac or
00:21:37 - 00:21:42: control slash on windows
00:21:40 - 00:21:44: like this now this line is a comment we
00:21:42 - 00:21:47: can press the same shortcut
00:21:44 - 00:21:48: one more time to remove the comment
00:21:47 - 00:21:50: so these were some of the most useful
00:21:48 - 00:21:51: shortcuts in jupyter
00:21:50 - 00:21:53: now over the next few lectures we're
00:21:51 - 00:21:54: going to work on a real machine learning
00:21:53 - 00:21:56: project
00:21:54 - 00:21:58: but before we get there let's delete all
00:21:56 - 00:22:01: the cells here so we start with
00:21:58 - 00:22:01: only a single empty cell so here in this
00:22:01 - 00:22:03: cell
00:22:01 - 00:22:05: first i'm going to press the escape
00:22:03 - 00:22:07: button now the cell is blue
00:22:05 - 00:22:09: so we are in the command mode and we can
00:22:07 - 00:22:12: delete the cell by pressing d
00:22:09 - 00:22:14: twice there you go now the next cell
00:22:12 - 00:22:17: is activated and it's in the command
00:22:14 - 00:22:19: mode so let's delete this as well
00:22:17 - 00:22:21: we have two more cells to delete there
00:22:19 - 00:22:24: you go and the last one
00:22:21 - 00:22:27: like this so now we have an empty
00:22:24 - 00:22:29: notebook with a single cell
00:22:27 - 00:22:31: hey guys i just wanted to let you know
00:22:29 - 00:22:32: that i have an online coding school at
00:22:31 - 00:22:34: cordwindmarch.com where you can find
00:22:32 - 00:22:35: plenty of courses on web and mobile
00:22:34 - 00:22:37: development
00:22:35 - 00:22:38: in fact i have a comprehensive python
00:22:37 - 00:22:41: course that teaches you everything about
00:22:38 - 00:22:41: python from the basics to more advanced
00:22:41 - 00:22:43: concepts
00:22:41 - 00:22:45: so after you watch this tutorial if you
00:22:43 - 00:22:46: want to learn more you may want to look
00:22:45 - 00:22:48: at my python course it comes with a 30
00:22:46 - 00:22:49: day money back guarantee
00:22:48 - 00:22:51: and a certificate of completion you can
00:22:49 - 00:22:52: add to your resume in case you're
00:22:51 - 00:22:55: interested
00:22:52 - 00:22:55: the link is below this video
00:22:58 - 00:23:01: over the next few lectures we're going
00:22:59 - 00:23:02: to work on a real machine learning
00:23:01 - 00:23:04: project
00:23:02 - 00:23:06: imagine we have an online music store
00:23:04 - 00:23:09: when our users sign up we ask their age
00:23:06 - 00:23:10: and gender and based on their profile we
00:23:09 - 00:23:12: recommend
00:23:10 - 00:23:14: various music albums they're likely to
00:23:12 - 00:23:15: buy so in this project we want to use
00:23:14 - 00:23:17: machine learning to
00:23:15 - 00:23:18: increase sales so we want to build a
00:23:17 - 00:23:21: model
00:23:18 - 00:23:23: we feed this model with some sample data
00:23:21 - 00:23:25: based on the existing users
00:23:23 - 00:23:26: our model will learn the patterns in our
00:23:25 - 00:23:28: data so we can ask it to make
00:23:26 - 00:23:30: predictions
00:23:28 - 00:23:32: when a user signs up we tell our model
00:23:30 - 00:23:34: hey we have a new user with this profile
00:23:32 - 00:23:35: what is the kind of music that this user
00:23:34 - 00:23:38: is interested in
00:23:35 - 00:23:38: our model will say jazz or hip hop or
00:23:38 - 00:23:39: whatever
00:23:38 - 00:23:42: and based on that we can make
00:23:39 - 00:23:44: suggestions to the user so this is the
00:23:42 - 00:23:45: problem we're going to solve
00:23:44 - 00:23:47: now back to the list of steps in a
00:23:45 - 00:23:49: machine learning project
00:23:47 - 00:23:51: first we need to import our data then we
00:23:49 - 00:23:52: should prepare or clean it
00:23:51 - 00:23:54: next we select a machine learning
00:23:52 - 00:23:56: algorithm to build a model
00:23:54 - 00:23:57: we train our model and ask it to make
00:23:56 - 00:24:00: predictions
00:23:57 - 00:24:01: and finally we evaluate our algorithm to
00:24:00 - 00:24:03: see its accuracy
00:24:01 - 00:24:04: if it's not accurate we either fine tune
00:24:03 - 00:24:07: our model
00:24:04 - 00:24:09: or select a different algorithm so let's
00:24:07 - 00:24:11: focus on the first step
00:24:09 - 00:24:13: download the csv file below this video
00:24:11 - 00:24:14: this is a very basic csv that i've
00:24:13 - 00:24:16: created for this project
00:24:14 - 00:24:18: it's just some random made up data it's
00:24:16 - 00:24:20: not real
00:24:18 - 00:24:22: so we have a table with three columns
00:24:20 - 00:24:25: age gender
00:24:22 - 00:24:26: and genre gender can either be one which
00:24:25 - 00:24:29: represents a male
00:24:26 - 00:24:30: or zero which represents a female here
00:24:29 - 00:24:33: i'm making a few assumptions
00:24:30 - 00:24:35: i'm assuming that men between 20 and 25
00:24:33 - 00:24:38: like hip-hop
00:24:35 - 00:24:40: men between 26 and 30 like jazz
00:24:38 - 00:24:42: and after the age of 30 they like
00:24:40 - 00:24:44: classical music
00:24:42 - 00:24:47: for women i'm assuming that if they're
00:24:44 - 00:24:49: between 20 and 25 they like dance music
00:24:47 - 00:24:51: if they're between 26 and 30 they like
00:24:49 - 00:24:53: acoustic music
00:24:51 - 00:24:54: and just like men after the age of 30
00:24:53 - 00:24:56: they like classical music
00:24:54 - 00:24:58: once again this is a made-up pattern
00:24:56 - 00:24:59: it's not the representation of the
00:24:58 - 00:25:01: reality
00:24:59 - 00:25:03: so let's go ahead and download this csv
00:25:01 - 00:25:07: click on this
00:25:03 - 00:25:10: dot dot icon here and download this file
00:25:07 - 00:25:11: in my downloads folder here we have this
00:25:10 - 00:25:13: music.csv
00:25:11 - 00:25:15: i'm going to drag and drop this onto
00:25:13 - 00:25:18: desktop because that's where
00:25:15 - 00:25:19: i've stored this hello world notebook so
00:25:18 - 00:25:22: i want you to put the csv file
00:25:19 - 00:25:24: right next to your jupyter notebook
00:25:22 - 00:25:26: now back to our notebook we need to read
00:25:24 - 00:25:28: the csv file
00:25:26 - 00:25:30: so just like before first we need to
00:25:28 - 00:25:34: import the pandas module
00:25:30 - 00:25:37: so import pandas as pd
00:25:34 - 00:25:38: and then we'll call pd that read analyze
00:25:37 - 00:25:42: csv
00:25:38 - 00:25:44: and the name of our file is music.csv
00:25:42 - 00:25:46: as you saw earlier this returns a data
00:25:44 - 00:25:48: frame which is a two-dimensional array
00:25:46 - 00:25:49: similar to an excel spreadsheet so let's
00:25:48 - 00:25:53: call that
00:25:49 - 00:25:56: music underline data
00:25:53 - 00:25:58: now let's inspect this music underline
00:25:56 - 00:25:59: data to make sure we loaded everything
00:25:58 - 00:26:02: properly
00:25:59 - 00:26:04: so run so here's our data frame
00:26:02 - 00:26:05: beautiful next minute to prepare or
00:26:04 - 00:26:08: clean the data
00:26:05 - 00:26:08: and that's the topic for the next
00:26:08 - 00:26:11: lecture
00:26:13 - 00:26:17: the second step in a machine learning
00:26:15 - 00:26:17: project is cleaning or preparing the
00:26:17 - 00:26:19: data
00:26:17 - 00:26:20: and that involves tasks such as removing
00:26:19 - 00:26:22: duplicates
00:26:20 - 00:26:24: null values and so on now in this
00:26:22 - 00:26:25: particular data set we don't have to do
00:26:24 - 00:26:26: any kind of cleaning because we don't
00:26:25 - 00:26:30: have any duplicates
00:26:26 - 00:26:32: and as you can see all rows have values
00:26:30 - 00:26:34: for all columns so we don't have null
00:26:32 - 00:26:35: values but there is one thing we need to
00:26:34 - 00:26:38: do
00:26:35 - 00:26:39: we should split this data set into two
00:26:38 - 00:26:41: separate data sets
00:26:39 - 00:26:43: one with the first two columns which we
00:26:41 - 00:26:45: refer to as the input set
00:26:43 - 00:26:47: and the other with the last column which
00:26:45 - 00:26:49: we refer to as the output set
00:26:47 - 00:26:51: so when we train a model we give it two
00:26:49 - 00:26:53: separate data sets
00:26:51 - 00:26:54: the input set and the output set the
00:26:53 - 00:26:57: output set
00:26:54 - 00:26:58: which is in this case the genre column
00:26:57 - 00:27:00: contains
00:26:58 - 00:27:01: the predictions so we're telling our
00:27:00 - 00:27:04: model that if we have
00:27:01 - 00:27:04: a user who's 20 years old and is a male
00:27:04 - 00:27:07: they like
00:27:04 - 00:27:09: hip hop once we train our model then we
00:27:07 - 00:27:11: give it a new input set
00:27:09 - 00:27:13: for example we say hey we have a new
00:27:11 - 00:27:15: user who is 21 years old
00:27:13 - 00:27:17: and is a male what is the genre of the
00:27:15 - 00:27:20: music that this user probably likes
00:27:17 - 00:27:22: as you can see in our input set we don't
00:27:20 - 00:27:24: have a sample for a 21 year old male
00:27:22 - 00:27:25: so we're going to ask our model to
00:27:24 - 00:27:27: predict that
00:27:25 - 00:27:30: that is the reason we need to split this
00:27:27 - 00:27:34: data set into two separate sets
00:27:30 - 00:27:36: input and output so back to our code
00:27:34 - 00:27:39: this data frame object has a method
00:27:36 - 00:27:42: called drop
00:27:39 - 00:27:43: now if you put the cursor under method
00:27:42 - 00:27:46: name and press
00:27:43 - 00:27:48: shift and tab you can see this tooltip
00:27:46 - 00:27:50: so this is the signature of this drop
00:27:48 - 00:27:52: method these are the parameters that we
00:27:50 - 00:27:53: can pass here
00:27:52 - 00:27:55: the parameter we're going to use in this
00:27:53 - 00:27:56: lecture is columns which is set to none
00:27:55 - 00:27:58: by default
00:27:56 - 00:28:00: with this parameter we can specify the
00:27:58 - 00:28:04: columns we want to drop
00:28:00 - 00:28:08: so in this case we set columns
00:28:04 - 00:28:10: to an array with one string genre
00:28:08 - 00:28:12: now this method doesn't actually modify
00:28:10 - 00:28:14: the original data set
00:28:12 - 00:28:15: in fact it will create a new data set
00:28:14 - 00:28:19: but without
00:28:15 - 00:28:20: this column so by convention we use a
00:28:19 - 00:28:23: capital x
00:28:20 - 00:28:26: to represent that data set so capital x
00:28:23 - 00:28:30: equals this expression now
00:28:26 - 00:28:32: let's inspect x so as you can
00:28:30 - 00:28:33: see our input set or x includes these
00:28:32 - 00:28:35: two columns
00:28:33 - 00:28:37: age and gender it doesn't have the
00:28:35 - 00:28:41: output or predictions
00:28:37 - 00:28:43: next we need to create our output set so
00:28:41 - 00:28:45: once again we start with our data frame
00:28:43 - 00:28:47: music data
00:28:45 - 00:28:49: using square brackets we can get all the
00:28:47 - 00:28:52: values in a given column
00:28:49 - 00:28:53: in this case genre once again this
00:28:52 - 00:28:55: returns
00:28:53 - 00:28:56: a new data set by convention we use a
00:28:55 - 00:28:59: lowercase y
00:28:56 - 00:29:00: to represent that so that is our output
00:28:59 - 00:29:03: data set
00:29:00 - 00:29:06: let's inspect that as well
00:29:03 - 00:29:07: so in this data set we only have the
00:29:06 - 00:29:10: predictions or
00:29:07 - 00:29:12: the answers so we have prepared our data
00:29:10 - 00:29:15: next we need to create a model using an
00:29:12 - 00:29:15: algorithm
00:29:19 - 00:29:22: the next step is to build a model using
00:29:21 - 00:29:24: a machine learning algorithm
00:29:22 - 00:29:27: there are so many algorithms out there
00:29:24 - 00:29:29: and each algorithm has its pros and cons
00:29:27 - 00:29:30: in terms of the performance and accuracy
00:29:29 - 00:29:32: in this lecture we're going to use a
00:29:30 - 00:29:34: very simple algorithm called
00:29:32 - 00:29:36: decision tree now the good news is that
00:29:34 - 00:29:38: we don't have to explicitly
00:29:36 - 00:29:40: program these algorithms they're already
00:29:38 - 00:29:45: implemented for us in a library called
00:29:40 - 00:29:49: scikit-learn so here on the top
00:29:45 - 00:29:52: from sklearn.3
00:29:49 - 00:29:56: let's import the decision
00:29:52 - 00:29:58: tree classifier so sklearn
00:29:56 - 00:30:00: is the package that comes with
00:29:58 - 00:30:01: scikit-learn library this is the most
00:30:00 - 00:30:02: popular machine learning library in
00:30:01 - 00:30:05: python
00:30:02 - 00:30:07: in this package we have a module called
00:30:05 - 00:30:07: tree and in this module we have a class
00:30:07 - 00:30:10: called
00:30:07 - 00:30:11: decision tree classifier this class
00:30:10 - 00:30:14: implements
00:30:11 - 00:30:16: the decision tree algorithm okay so
00:30:14 - 00:30:18: now we need to create a new instance of
00:30:16 - 00:30:21: this class
00:30:18 - 00:30:21: so at the end let's create an object
00:30:21 - 00:30:24: called
00:30:21 - 00:30:26: model and set it to a new instance of
00:30:24 - 00:30:29: decision
00:30:26 - 00:30:32: tree classifier
00:30:29 - 00:30:33: like this so now we have a model next we
00:30:32 - 00:30:35: need to train it so it learns
00:30:33 - 00:30:37: patterns in the data and that is pretty
00:30:35 - 00:30:40: easy we call
00:30:37 - 00:30:41: model that fit this method takes two
00:30:40 - 00:30:44: data sets
00:30:41 - 00:30:48: the input set and the output set so
00:30:44 - 00:30:50: they are capital x and y
00:30:48 - 00:30:52: now finally we need to ask our model to
00:30:50 - 00:30:53: make a prediction
00:30:52 - 00:30:56: so we can ask it what is the kind of
00:30:53 - 00:30:59: music that a 21 year old male likes
00:30:56 - 00:31:00: now before we do that let's temporarily
00:30:59 - 00:31:04: inspect
00:31:00 - 00:31:07: our initial data set that is music data
00:31:04 - 00:31:09: so look what we got here
00:31:07 - 00:31:10: as i told you earlier i've assumed that
00:31:09 - 00:31:13: men between 20
00:31:10 - 00:31:14: and 25 like hip-hop music but here we
00:31:13 - 00:31:17: only have
00:31:14 - 00:31:20: three samples for men aged 20
00:31:17 - 00:31:22: 23 and 25 we don't have a sample for a
00:31:20 - 00:31:23: 21 year old male
00:31:22 - 00:31:25: so if you ask our model to predict the
00:31:23 - 00:31:27: kind of music that a 21 year old male
00:31:25 - 00:31:30: likes we expect it to say
00:31:27 - 00:31:32: hip hop similarly i've assumed that
00:31:30 - 00:31:34: women between 20 and 25
00:31:32 - 00:31:36: like dance music but we don't have a
00:31:34 - 00:31:38: sample for a 22 year old female
00:31:36 - 00:31:40: so once again if you ask our model to
00:31:38 - 00:31:42: predict the kind of music
00:31:40 - 00:31:43: that a 22 year old woman likes we expect
00:31:42 - 00:31:46: it to say
00:31:43 - 00:31:48: dance so with these assumptions
00:31:46 - 00:31:50: let's go ahead and ask our model to make
00:31:48 - 00:31:53: predictions
00:31:50 - 00:31:56: so let's remove
00:31:53 - 00:31:56: the last line and instead we're going to
00:31:56 - 00:32:00: call
00:31:56 - 00:32:03: model dot predict this method
00:32:00 - 00:32:04: takes a two dimensional array so here's
00:32:03 - 00:32:07: the outer array
00:32:04 - 00:32:08: in this array each element is an array
00:32:07 - 00:32:10: so i'm going to pass
00:32:08 - 00:32:11: another array here and in this array i'm
00:32:10 - 00:32:14: going to pass
00:32:11 - 00:32:17: a new input set a 21 year old male
00:32:14 - 00:32:18: so 21 comma one that is like a new
00:32:17 - 00:32:22: record
00:32:18 - 00:32:22: in this table okay so this is one input
00:32:22 - 00:32:24: set
00:32:22 - 00:32:26: let's pass another input set for a
00:32:24 - 00:32:29: 22-year female
00:32:26 - 00:32:32: so here's another array here we add 22
00:32:29 - 00:32:34: comma zero so we're asking our model to
00:32:32 - 00:32:38: make two predictions at the same time
00:32:34 - 00:32:39: we get the result and store it in a
00:32:38 - 00:32:43: variable called predictions
00:32:39 - 00:32:44: and finally let's inspect that in our
00:32:43 - 00:32:48: notebook
00:32:44 - 00:32:50: run look what we got
00:32:48 - 00:32:51: our model is saying that a 21 year old
00:32:50 - 00:32:53: male
00:32:51 - 00:32:54: likes hip hop and a 22 year old female
00:32:53 - 00:32:56: likes
00:32:54 - 00:32:58: dance music so our model could
00:32:56 - 00:33:00: successfully make predictions here
00:32:58 - 00:33:02: beautiful but wait a second building a
00:33:00 - 00:33:03: model that makes predictions accurately
00:33:02 - 00:33:05: is not always that easy
00:33:03 - 00:33:08: as i told you earlier after we build a
00:33:05 - 00:33:10: model we need to measure its accuracy
00:33:08 - 00:33:11: and if it's not accurate enough we
00:33:10 - 00:33:13: should either fine tune it or build a
00:33:11 - 00:33:15: model using a different algorithm
00:33:13 - 00:33:19: so in the next lecture i'm going to show
00:33:15 - 00:33:19: you how to measure the accuracy of a
00:33:22 - 00:33:25: model
00:33:24 - 00:33:27: in this lecture i'm going to show you
00:33:25 - 00:33:28: how to measure the accuracy of your
00:33:27 - 00:33:30: models
00:33:28 - 00:33:31: now in order to do so first we need to
00:33:30 - 00:33:34: split our data set
00:33:31 - 00:33:35: into two sets one for training and the
00:33:34 - 00:33:37: other for testing
00:33:35 - 00:33:38: because right now we're passing the
00:33:37 - 00:33:40: entire data set
00:33:38 - 00:33:42: for training the model and we're using
00:33:40 - 00:33:44: two samples
00:33:42 - 00:33:46: for making predictions that is not
00:33:44 - 00:33:46: enough to calculate the accuracy of a
00:33:46 - 00:33:49: model
00:33:46 - 00:33:51: a general rule of thumb is to allocate
00:33:49 - 00:33:53: 70 to 80 percent of our data
00:33:51 - 00:33:54: for training and the other twenty to
00:33:53 - 00:33:57: thirty percent for testing
00:33:54 - 00:33:58: then instead of passing only two samples
00:33:57 - 00:34:00: for making predictions
00:33:58 - 00:34:02: we can pass the data set we have for
00:34:00 - 00:34:03: testing we'll get the predictions
00:34:02 - 00:34:06: and then we can compare these
00:34:03 - 00:34:08: predictions with the actual values
00:34:06 - 00:34:09: in the test set based on that we can
00:34:08 - 00:34:11: calculate the accuracy
00:34:09 - 00:34:13: that's really easy all we have to do is
00:34:11 - 00:34:15: to import a couple of functions
00:34:13 - 00:34:18: and call them in this code let me show
00:34:15 - 00:34:21: you so first on the top
00:34:18 - 00:34:24: from sklearn the model
00:34:21 - 00:34:28: underline selection module we import
00:34:24 - 00:34:30: a function called train test split
00:34:28 - 00:34:32: with this function we can easily split
00:34:30 - 00:34:35: our data set into two sets
00:34:32 - 00:34:39: for training and testing now
00:34:35 - 00:34:40: right here after we define x and y sets
00:34:39 - 00:34:44: we call this function
00:34:40 - 00:34:47: so train test split
00:34:44 - 00:34:50: we give it three arguments x y
00:34:47 - 00:34:52: and a keyboard argument that specifies
00:34:50 - 00:34:57: the size of our test data set
00:34:52 - 00:34:59: so test underline size we set it to 0.2
00:34:57 - 00:35:00: so we are allocating 20 of our data for
00:34:59 - 00:35:03: testing
00:35:00 - 00:35:05: now this function returns a tuple so we
00:35:03 - 00:35:09: can unpack it into four variables
00:35:05 - 00:35:13: right here x underline train
00:35:09 - 00:35:16: x underline test y underline train
00:35:13 - 00:35:19: and y underline test
00:35:16 - 00:35:20: so the first two variables are the input
00:35:19 - 00:35:24: sets for training
00:35:20 - 00:35:24: and testing and the other are the output
00:35:24 - 00:35:27: sets
00:35:24 - 00:35:29: for training and testing now when
00:35:27 - 00:35:31: training our model
00:35:29 - 00:35:32: instead of passing the entire data set
00:35:31 - 00:35:36: we want to pass only
00:35:32 - 00:35:39: the training data set so x
00:35:36 - 00:35:42: underline train and y
00:35:39 - 00:35:42: underline train also when making
00:35:42 - 00:35:44: predictions
00:35:42 - 00:35:46: instead of passing these two samples we
00:35:44 - 00:35:49: pass x
00:35:46 - 00:35:50: underline test so that is the data set
00:35:49 - 00:35:53: that contains
00:35:50 - 00:35:55: input values for testing now we get
00:35:53 - 00:35:57: the predictions to calculate the
00:35:55 - 00:35:58: accuracy we simply have to compare these
00:35:57 - 00:36:01: predictions
00:35:58 - 00:36:02: with the actual values we have in our
00:36:01 - 00:36:05: output set
00:36:02 - 00:36:06: for testing that is very easy first on
00:36:05 - 00:36:10: the top we need to import
00:36:06 - 00:36:11: a function so from sklearn.metrics
00:36:10 - 00:36:16: metrics
00:36:11 - 00:36:17: import accuracy underlying score
00:36:16 - 00:36:20: now at the very end we call this
00:36:17 - 00:36:23: function so accuracy
00:36:20 - 00:36:26: score and
00:36:23 - 00:36:27: give it two arguments y underline test
00:36:26 - 00:36:31: which contains
00:36:27 - 00:36:34: the expected values and predictions
00:36:31 - 00:36:35: which contains the actual values now
00:36:34 - 00:36:38: this function returns
00:36:35 - 00:36:41: an accuracy score between zero to one so
00:36:38 - 00:36:44: we can store it here
00:36:41 - 00:36:48: and simply display it on the console
00:36:44 - 00:36:50: so let's go ahead and run this program
00:36:48 - 00:36:51: so the accuracy score is one or 100
00:36:50 - 00:36:53: percent
00:36:51 - 00:36:55: but if we run this one more time we're
00:36:53 - 00:36:57: going to see a different result
00:36:55 - 00:36:58: because every time we split our data set
00:36:57 - 00:37:00: into training and test sets
00:36:58 - 00:37:02: we'll have different data sets because
00:37:00 - 00:37:05: this function randomly picks data
00:37:02 - 00:37:06: for training and testing let me show you
00:37:05 - 00:37:08: so put the cursor
00:37:06 - 00:37:10: in the cell now you can see this cell is
00:37:08 - 00:37:11: activated note that if you click this
00:37:10 - 00:37:14: button here
00:37:11 - 00:37:15: it will run this cell and also inserts a
00:37:14 - 00:37:17: new cell
00:37:15 - 00:37:19: below this cell let me show you so if i
00:37:17 - 00:37:21: go to the second cell
00:37:19 - 00:37:22: press escape button now we are in the
00:37:21 - 00:37:26: command mode
00:37:22 - 00:37:29: press d twice okay now it's deleted
00:37:26 - 00:37:31: if we click the run button you can see
00:37:29 - 00:37:32: this code was executed and now we have a
00:37:31 - 00:37:34: new cell
00:37:32 - 00:37:36: so if you want to run our first cell
00:37:34 - 00:37:36: multiple times every time we have to
00:37:36 - 00:37:38: click this
00:37:36 - 00:37:40: and then run it and then click again and
00:37:38 - 00:37:42: run it it's a little bit tedious
00:37:40 - 00:37:44: so i'll show you a shortcut activate the
00:37:42 - 00:37:47: first cell
00:37:44 - 00:37:49: and press ctrl and enter
00:37:47 - 00:37:50: this runs the current cell without
00:37:49 - 00:37:53: adding a new cell
00:37:50 - 00:37:54: below it so back here let's run it
00:37:53 - 00:37:57: multiple times
00:37:54 - 00:37:58: okay now look the accuracy dropped to
00:37:57 - 00:38:00: 0.75
00:37:58 - 00:38:02: it's still good so the accuracy score
00:38:00 - 00:38:06: here is somewhere between
00:38:02 - 00:38:10: 75 to 100 but let me show you something
00:38:06 - 00:38:13: if i change the test size from 0.2
00:38:10 - 00:38:13: to 0.8 so essentially we're using only
00:38:13 - 00:38:16: 20
00:38:13 - 00:38:17: of our data for training this model and
00:38:16 - 00:38:20: we're using the other 80
00:38:17 - 00:38:22: for testing now let's see what happens
00:38:20 - 00:38:25: when we run this cell multiple times
00:38:22 - 00:38:27: so control and enter look the accuracy
00:38:25 - 00:38:30: immediately dropped to 0.4
00:38:27 - 00:38:35: one more time now 46 percent
00:38:30 - 00:38:37: 40 26 it's really really bad
00:38:35 - 00:38:39: the reason this is happening is because
00:38:37 - 00:38:41: we are using very little data
00:38:39 - 00:38:43: for training this model this is one of
00:38:41 - 00:38:45: the key concepts in machine learning
00:38:43 - 00:38:46: the more data we give to our model and
00:38:45 - 00:38:49: the cleaner the data is
00:38:46 - 00:38:50: we get the better result so if we have
00:38:49 - 00:38:52: duplicates
00:38:50 - 00:38:53: irrelevant data or incomplete values our
00:38:52 - 00:38:55: model will learn
00:38:53 - 00:38:57: bad patterns in our data that is why
00:38:55 - 00:39:00: it's really important to clean our data
00:38:57 - 00:39:03: before training our model now let's
00:39:00 - 00:39:05: change this back to 0.2
00:39:03 - 00:39:06: run this one more time okay now the
00:39:05 - 00:39:10: accuracy is one
00:39:06 - 00:39:11: 75 percent now we drop to 50
00:39:10 - 00:39:14: again the reason this is happening is
00:39:11 - 00:39:16: because we don't have enough data
00:39:14 - 00:39:18: some machine learning problems require
00:39:16 - 00:39:20: thousands or even millions of samples
00:39:18 - 00:39:22: to train a model the more complex the
00:39:20 - 00:39:24: problem is the more data we need
00:39:22 - 00:39:26: for example here we're only dealing with
00:39:24 - 00:39:28: a table of three columns
00:39:26 - 00:39:30: but if you want to build a model to tell
00:39:28 - 00:39:31: if a picture is a cat or a dog or a
00:39:30 - 00:39:33: horse or a lion
00:39:31 - 00:39:34: we'll need millions of pictures the more
00:39:33 - 00:39:36: animals we want to support the more
00:39:34 - 00:39:37: pictures we need
00:39:36 - 00:39:43: in the next lecture we're going to talk
00:39:37 - 00:39:43: about model persistence
00:39:45 - 00:39:49: so this is a very basic implementation
00:39:47 - 00:39:51: of building and training a model to make
00:39:49 - 00:39:53: predictions
00:39:51 - 00:39:54: now to simplify things i have removed
00:39:53 - 00:39:55: all the code that we wrote in the last
00:39:54 - 00:39:57: lecture for
00:39:55 - 00:39:58: calculating the accuracy because in this
00:39:57 - 00:40:01: lecture we're going to focus
00:39:58 - 00:40:03: on a different topic so basically we
00:40:01 - 00:40:06: import our data set
00:40:03 - 00:40:08: create a model train it and then
00:40:06 - 00:40:10: ask it to make predictions now this
00:40:08 - 00:40:12: piece of code that you see
00:40:10 - 00:40:14: here is not what we want to run every
00:40:12 - 00:40:16: time we have a new user or every time we
00:40:14 - 00:40:18: want to make recommendations
00:40:16 - 00:40:19: to an existing user because training a
00:40:18 - 00:40:20: model can sometimes be really time
00:40:19 - 00:40:22: consuming
00:40:20 - 00:40:23: in this example we're dealing with a
00:40:22 - 00:40:26: very small data set that has
00:40:23 - 00:40:27: only 20 records but in real applications
00:40:26 - 00:40:30: we might have a data set
00:40:27 - 00:40:31: with thousands or millions of samples
00:40:30 - 00:40:32: training a model for that might take
00:40:31 - 00:40:35: seconds
00:40:32 - 00:40:37: minutes or even hours so that is why
00:40:35 - 00:40:39: model persistence is important
00:40:37 - 00:40:42: once in a while we build and train our
00:40:39 - 00:40:43: model and then we'll save it to a file
00:40:42 - 00:40:45: now next time we want to make
00:40:43 - 00:40:46: predictions we simply load the model
00:40:45 - 00:40:49: from the file and
00:40:46 - 00:40:51: ask it to make predictions that model is
00:40:49 - 00:40:51: already trained we don't need to retrain
00:40:51 - 00:40:54: it
00:40:51 - 00:40:55: it's like an intelligent person so let
00:40:54 - 00:40:57: me show you how to do this it's very
00:40:55 - 00:41:02: very easy
00:40:57 - 00:41:05: on the top from sklearn.externals
00:41:02 - 00:41:08: module we import
00:41:05 - 00:41:09: lib this job live object has methods for
00:41:08 - 00:41:12: saving
00:41:09 - 00:41:15: and loading models so after
00:41:12 - 00:41:18: we train our model we simply call
00:41:15 - 00:41:21: joblib dot dump
00:41:18 - 00:41:22: and give it two arguments our model and
00:41:21 - 00:41:24: the name of the file
00:41:22 - 00:41:25: in which we want to store this model
00:41:24 - 00:41:28: let's call that
00:41:25 - 00:41:33: music dash recommender
00:41:28 - 00:41:34: dot job lib that's all we have to do
00:41:33 - 00:41:36: now temporarily i'm going to comment out
00:41:34 - 00:41:38: this line we don't want to make any
00:41:36 - 00:41:40: predictions we just want to store our
00:41:38 - 00:41:42: trained model in a file
00:41:40 - 00:41:44: so let's run this cell with control and
00:41:42 - 00:41:48: slash
00:41:44 - 00:41:50: okay look in the output we have an array
00:41:48 - 00:41:52: that contains the name of our model file
00:41:50 - 00:41:53: so this is the return value of the dump
00:41:52 - 00:41:55: method
00:41:53 - 00:41:57: now back to our desktop right next to my
00:41:55 - 00:41:59: notebook you can see our job live file
00:41:57 - 00:42:01: this is where our model is stored it's
00:41:59 - 00:42:04: simply a binary file
00:42:01 - 00:42:05: now back to our jupyter notebook as i
00:42:04 - 00:42:07: told you before in a real application we
00:42:05 - 00:42:10: don't want to train a model
00:42:07 - 00:42:12: every time so let's comment out
00:42:10 - 00:42:14: these few lines so i've selected these
00:42:12 - 00:42:17: few lines on mac we can press
00:42:14 - 00:42:18: command and slash on windows control
00:42:17 - 00:42:20: slash
00:42:18 - 00:42:23: okay these lines are commented out now
00:42:20 - 00:42:24: this time instead of dumping our model
00:42:23 - 00:42:27: we're going to load it so we call the
00:42:24 - 00:42:28: load method we don't have the model we
00:42:27 - 00:42:31: simply pass
00:42:28 - 00:42:32: the name of our model file this returns
00:42:31 - 00:42:35: our trained
00:42:32 - 00:42:35: model now with these two lines we can
00:42:35 - 00:42:38: simply
00:42:35 - 00:42:40: make predictions so earlier we assumed
00:42:38 - 00:42:43: that men between 20 and 25
00:42:40 - 00:42:45: like hip-hop music let's print
00:42:43 - 00:42:47: predictions and see if our model is
00:42:45 - 00:42:50: behaving correctly or not
00:42:47 - 00:42:54: so control and enter there you go
00:42:50 - 00:42:54: so this is how we persist and load
00:42:56 - 00:42:59: models
00:42:59 - 00:43:03: earlier in this section i told you that
00:43:01 - 00:43:04: decision trees are the easiest to
00:43:03 - 00:43:05: understand and that's why we started
00:43:04 - 00:43:07: machine learning
00:43:05 - 00:43:09: with decision trees in this lecture
00:43:07 - 00:43:10: we're going to export our model in a
00:43:09 - 00:43:12: visual format
00:43:10 - 00:43:13: so you will see how this model makes
00:43:12 - 00:43:15: predictions
00:43:13 - 00:43:17: that is really really cool let me show
00:43:15 - 00:43:19: you so
00:43:17 - 00:43:21: once again i've simplified this code so
00:43:19 - 00:43:24: we simply import
00:43:21 - 00:43:25: our data set create input and output
00:43:24 - 00:43:28: sets
00:43:25 - 00:43:30: create a model and train it
00:43:28 - 00:43:32: that's all we are doing now i want you
00:43:30 - 00:43:34: to follow along with me
00:43:32 - 00:43:35: type everything exactly as i show you in
00:43:34 - 00:43:36: this lecture don't worry about
00:43:35 - 00:43:39: what everything means we're going to
00:43:36 - 00:43:42: come back to it shortly so
00:43:39 - 00:43:45: on the top from sklearn
00:43:42 - 00:43:48: import tree this object
00:43:45 - 00:43:50: has a method for exporting our decision
00:43:48 - 00:43:53: tree in a graphical format
00:43:50 - 00:43:56: so after we train our model let's call
00:43:53 - 00:43:59: tree dot export underline
00:43:56 - 00:44:00: graph vis now here are a few arguments
00:43:59 - 00:44:03: we need to pass
00:44:00 - 00:44:05: the first argument is our model the
00:44:03 - 00:44:07: second is the name of the output file
00:44:05 - 00:44:09: so here we're going to use keyword
00:44:07 - 00:44:10: arguments because this method takes so
00:44:09 - 00:44:11: many parameters and we want to
00:44:10 - 00:44:13: selectively pass
00:44:11 - 00:44:14: keyword arguments without worrying about
00:44:13 - 00:44:17: their order
00:44:14 - 00:44:18: so the parameter we're going to set is
00:44:17 - 00:44:21: out
00:44:18 - 00:44:25: underline file let's set this to
00:44:21 - 00:44:28: music dash recommender dot
00:44:25 - 00:44:28: d o t this is the dot format which is a
00:44:28 - 00:44:30: graph
00:44:28 - 00:44:31: description language you'll see that
00:44:30 - 00:44:34: shortly
00:44:31 - 00:44:35: the other parameter we want to set is
00:44:34 - 00:44:38: feature
00:44:35 - 00:44:41: underline names we set this to an array
00:44:38 - 00:44:43: of two strings age and
00:44:41 - 00:44:44: gender these are the features or the
00:44:43 - 00:44:46: columns
00:44:44 - 00:44:48: of our data set so they are the
00:44:46 - 00:44:52: properties or features of our data
00:44:48 - 00:44:55: okay the other parameter is class names
00:44:52 - 00:44:57: so class underline names we should set
00:44:55 - 00:45:00: this to the list of classes
00:44:57 - 00:45:04: or labels we have in our output data set
00:45:00 - 00:45:06: like hip hop jazz classical and so on
00:45:04 - 00:45:09: so this y data set includes all the
00:45:06 - 00:45:11: genres or all the classes of our data
00:45:09 - 00:45:13: but they're repeated a few times in this
00:45:11 - 00:45:16: data set so
00:45:13 - 00:45:19: here we call y dot unique this returns
00:45:16 - 00:45:20: the unique list of classes now we should
00:45:19 - 00:45:24: sort this alphabetically
00:45:20 - 00:45:28: so we call the sorted function and
00:45:24 - 00:45:32: pass the result a y dot unique
00:45:28 - 00:45:33: the next parameter is label we set this
00:45:32 - 00:45:35: to a string
00:45:33 - 00:45:37: all once again don't worry about the
00:45:35 - 00:45:39: details of these parameters we're going
00:45:37 - 00:45:42: to come back to this shortly
00:45:39 - 00:45:45: so set label to all then
00:45:42 - 00:45:50: round it to true and finally
00:45:45 - 00:45:52: filled to true so this is the end result
00:45:50 - 00:45:53: now let's run this cell using control
00:45:52 - 00:45:57: and enter
00:45:53 - 00:45:59: okay here we have a new file
00:45:57 - 00:46:00: music recommender dot dot that's a
00:45:59 - 00:46:03: little bit funny
00:46:00 - 00:46:05: so we want to open this file with vs
00:46:03 - 00:46:08: code so drag and drop this
00:46:05 - 00:46:08: into a vs code window
00:46:10 - 00:46:15: okay here's a dot format it's a textual
00:46:12 - 00:46:17: language for describing graphs
00:46:15 - 00:46:19: now to visualize this graph we need to
00:46:17 - 00:46:22: install an extension in vs code
00:46:19 - 00:46:22: so on the left side click the extensions
00:46:22 - 00:46:26: panel
00:46:22 - 00:46:28: and search for dot dot
00:46:26 - 00:46:30: look at the second extension here
00:46:28 - 00:46:33: graphviz or dot
00:46:30 - 00:46:35: language by staphon vs
00:46:33 - 00:46:37: go ahead and install this extension and
00:46:35 - 00:46:39: then reload vs code
00:46:37 - 00:46:40: once you do that you can visualize this
00:46:39 - 00:46:44: dot file
00:46:40 - 00:46:46: so let me close this tab all right
00:46:44 - 00:46:47: look at this dot dot here on the right
00:46:46 - 00:46:49: side click this
00:46:47 - 00:46:50: you should have a new menu open preview
00:46:49 - 00:46:53: to the site
00:46:50 - 00:46:54: so click that all right here's the
00:46:53 - 00:46:57: visualization
00:46:54 - 00:46:58: of our decision tree let's close the dot
00:46:57 - 00:47:00: file
00:46:58 - 00:47:02: there you go this is exactly how our
00:47:00 - 00:47:04: model makes predictions
00:47:02 - 00:47:06: so we have this binary tree which means
00:47:04 - 00:47:09: every node can have
00:47:06 - 00:47:10: a maximum of two children on top of each
00:47:09 - 00:47:12: node we have
00:47:10 - 00:47:14: a condition if this condition is true we
00:47:12 - 00:47:16: go to the child node on the left side
00:47:14 - 00:47:18: otherwise we go to the child node on the
00:47:16 - 00:47:19: right side so let's see what's happening
00:47:18 - 00:47:22: here
00:47:19 - 00:47:24: the first condition is age less than or
00:47:22 - 00:47:26: equal to 30.5
00:47:24 - 00:47:28: if this condition is false that means
00:47:26 - 00:47:30: that user is 30 years or older
00:47:28 - 00:47:32: so the genre of the music that they're
00:47:30 - 00:47:35: interested in is classical
00:47:32 - 00:47:36: so here we're classifying people based
00:47:35 - 00:47:38: on their profile
00:47:36 - 00:47:41: that is the reason we have the word
00:47:38 - 00:47:41: class here so a user who is 30 years or
00:47:41 - 00:47:44: older
00:47:41 - 00:47:45: belongs to the class of classical or
00:47:44 - 00:47:47: people who like
00:47:45 - 00:47:49: classical music now what if this
00:47:47 - 00:47:52: condition is true
00:47:49 - 00:47:55: that means that user is younger than 30.
00:47:52 - 00:47:55: so now we check the gender if it's less
00:47:55 - 00:47:58: than
00:47:55 - 00:47:59: 0.5 which basically means if it equals
00:47:58 - 00:48:01: to 0
00:47:59 - 00:48:03: then we're dealing with a female so we
00:48:01 - 00:48:05: go to the child node here
00:48:03 - 00:48:07: now once again we have another condition
00:48:05 - 00:48:08: so we're dealing with a female who is
00:48:07 - 00:48:11: younger than 30.
00:48:08 - 00:48:12: once again we need to check their age so
00:48:11 - 00:48:16: is the age
00:48:12 - 00:48:18: less than 25.5 if that's the case then
00:48:16 - 00:48:21: that user likes dance music otherwise
00:48:18 - 00:48:22: they like acoustic music
00:48:21 - 00:48:24: so this is the decision tree that our
00:48:22 - 00:48:26: model uses to make
00:48:24 - 00:48:28: predictions now if you're wondering why
00:48:26 - 00:48:31: we have these floating point numbers
00:48:28 - 00:48:32: like 25.5 these are basically the rules
00:48:31 - 00:48:34: that our model generates
00:48:32 - 00:48:36: based on the patterns that it finds in
00:48:34 - 00:48:38: our data set
00:48:36 - 00:48:40: as we give our model more data these
00:48:38 - 00:48:41: rules will change so they're not always
00:48:40 - 00:48:43: the same
00:48:41 - 00:48:45: also the more columns or more features
00:48:43 - 00:48:46: we have our decision tree is going to
00:48:45 - 00:48:48: get more complex
00:48:46 - 00:48:50: currently we have only two features age
00:48:48 - 00:48:51: and gender
00:48:50 - 00:48:53: now back to our code let me quickly
00:48:51 - 00:48:54: explain the meaning of all these
00:48:53 - 00:48:56: parameters
00:48:54 - 00:48:58: we set fill to true so each box or each
00:48:56 - 00:49:01: node is filled with a color
00:48:58 - 00:49:03: we set rounded to true so they have
00:49:01 - 00:49:05: rounded corners
00:49:03 - 00:49:08: we set label to all so every node has
00:49:05 - 00:49:10: labels that we can read
00:49:08 - 00:49:11: we set class names to the unique list of
00:49:10 - 00:49:12: genres
00:49:11 - 00:49:14: and that's for displaying the class for
00:49:12 - 00:49:16: each node right here
00:49:14 - 00:49:17: and we set feature names to age and
00:49:16 - 00:49:21: gender
00:49:17 - 00:49:21: so we can see the rules in our notes
00:49:22 - 00:49:25: hey thank you for watching my tutorial i
00:49:24 - 00:49:27: hope you learned a lot and you're
00:49:25 - 00:49:29: excited to learn more
00:49:27 - 00:49:30: if you enjoyed this tutorial please like
00:49:29 - 00:49:31: and share it with others and be sure to
00:49:30 - 00:49:34: subscribe to my channel
00:49:31 - 00:49:36: as i upload new videos every week once
00:49:34 - 00:49:39: again thank you and i wish you all the
00:49:36 - 00:49:39: [Music]
00:49:41 - 00:49:44: best
