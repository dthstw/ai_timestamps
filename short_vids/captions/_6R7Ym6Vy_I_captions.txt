00:00:00 - 00:00:03: (gentle music jingle)
00:00:03 - 00:00:06: (audience applauding)
00:00:12 - 00:00:14: - Whoa, so many of you.
00:00:14 - 00:00:17: Good, okay, thank you for
that lovely introduction.
00:00:19 - 00:00:24: Right, so, what is generative
artificial intelligence?
00:00:24 - 00:00:27: So I'm gonna explain what
artificial intelligence is
00:00:27 - 00:00:30: and I want this to be a bit interactive
00:00:30 - 00:00:33: so there will be some
audience participation.
00:00:33 - 00:00:36: The people here who hold
this lecture said to me,
00:00:36 - 00:00:40: "Oh, you are very low-tech
for somebody working on AI."
00:00:40 - 00:00:42: I don't have any explosions
or any experiments,
00:00:42 - 00:00:45: so I'm afraid you'll have to participate,
00:00:45 - 00:00:46: I hope that's okay.
00:00:46 - 00:00:50: All right, so, what is generative
artificial intelligence?
00:00:50 - 00:00:55: So the term is made up by two things,
00:00:55 - 00:00:57: artificial intelligence and generative.
00:00:57 - 00:01:02: So artificial intelligence
is a fancy term for saying
00:01:02 - 00:01:05: we get a computer programme to do the job
00:01:05 - 00:01:07: that a human would otherwise do.
00:01:07 - 00:01:09: And generative, this is the fun bit,
00:01:09 - 00:01:12: we are creating new content
00:01:12 - 00:01:15: that the computer has
not necessarily seen,
00:01:15 - 00:01:17: it has seen parts of it,
00:01:17 - 00:01:21: and it's able to synthesise
it and give us new things.
00:01:21 - 00:01:23: So what would this new content be?
00:01:23 - 00:01:25: It could be audio,
00:01:25 - 00:01:27: it could be computer code
00:01:27 - 00:01:29: so that it writes a programme for us,
00:01:29 - 00:01:31: it could be a new image,
00:01:31 - 00:01:32: it could be a text,
00:01:32 - 00:01:36: like an email or an essay
you've heard, or video.
00:01:36 - 00:01:37: Now in this lecture
00:01:37 - 00:01:41: I'm only gonna be mostly focusing on text
00:01:41 - 00:01:42: because I do natural language processing
00:01:42 - 00:01:44: and this is what I know about,
00:01:44 - 00:01:48: and we'll see how the technology works
00:01:48 - 00:01:53: and hopefully leaving the
lecture you'll know how,
00:01:53 - 00:01:57: like there's a lot of myth
around it and it's not,
00:01:57 - 00:02:00: you'll see what it does
and it's just a tool, okay?
00:02:02 - 00:02:03: Right, so the outline of the talk,
00:02:03 - 00:02:05: there's three parts and
it's kind of boring.
00:02:05 - 00:02:08: This is Alice Morse Earle.
00:02:08 - 00:02:11: I do not expect that you know the lady.
00:02:11 - 00:02:13: She was an American writer
00:02:13 - 00:02:18: and she writes about
memorabilia and customs,
00:02:18 - 00:02:21: but she's famous for her quotes.
00:02:21 - 00:02:23: So she's given us this
quote here that says,
00:02:23 - 00:02:25: "Yesterday's history,
tomorrow is a mystery,
00:02:25 - 00:02:28: today is a gift, and that's
why it's called the present."
00:02:28 - 00:02:29: It's a very optimistic quote.
00:02:29 - 00:02:32: And the lecture is basically
00:02:32 - 00:02:37: the past, the present,
and the future of AI.
00:02:37 - 00:02:41: Okay, so what I want to
say right at the front
00:02:41 - 00:02:46: is that generative AI
is not a new concept.
00:02:46 - 00:02:49: It's been around for a while.
00:02:49 - 00:02:54: So how many of you have
used or are familiar
00:02:54 - 00:02:56: with Google Translate?
00:02:56 - 00:02:58: Can I see a show of hands?
00:02:58 - 00:03:02: Right, who can tell me when
Google Translate launched
00:03:02 - 00:03:03: for the first time?
00:03:05 - 00:03:08: - 1995?
- Oh, that would've been good.
00:03:08 - 00:03:13: 2006, so it's been around for 17 years
00:03:14 - 00:03:16: and we've all been using it.
00:03:16 - 00:03:18: And this is an example of generative AI.
00:03:18 - 00:03:21: Greek text comes in,
I'm Greek, so you know,
00:03:21 - 00:03:24: pay some juice to the... (laughs)
00:03:24 - 00:03:26: Right, so Greek text comes in,
00:03:27 - 00:03:29: English text comes out.
00:03:29 - 00:03:31: And Google Translate
has served us very well
00:03:31 - 00:03:32: for all these years
00:03:32 - 00:03:34: and nobody was making a fuss.
00:03:35 - 00:03:40: Another example is Siri on the phone.
00:03:40 - 00:03:43: Again, Siri launched 2011,
00:03:46 - 00:03:48: 12 years ago,
00:03:48 - 00:03:51: and it was a sensation back then.
00:03:51 - 00:03:53: It is another example of generative AI.
00:03:53 - 00:03:58: We can ask Siri to set
alarms and Siri talks back
00:03:58 - 00:04:00: and oh how great it is
00:04:00 - 00:04:02: and then you can ask about
your alarms and whatnot.
00:04:02 - 00:04:03: This is generative AI.
00:04:03 - 00:04:06: Again, it's not as
sophisticated as ChatGPT,
00:04:06 - 00:04:07: but it was there.
00:04:07 - 00:04:10: And I don't know how many have an iPhone?
00:04:11 - 00:04:15: See, iPhones are quite
popular, I don't know why.
00:04:15 - 00:04:19: Okay, so, we are all familiar with that.
00:04:19 - 00:04:22: And of course later on there
was Amazon Alexa and so on.
00:04:23 - 00:04:27: Okay, again, generative
AI is not a new concept,
00:04:27 - 00:04:31: it is everywhere, it
is part of your phone.
00:04:31 - 00:04:34: The completion when
you're sending an email
00:04:34 - 00:04:36: or when you're sending a text.
00:04:36 - 00:04:40: The phone attempts to
complete your sentences,
00:04:40 - 00:04:44: attempts to think like you
and it saves you time, right?
00:04:44 - 00:04:46: Because some of the completions are there.
00:04:46 - 00:04:47: The same with Google,
00:04:47 - 00:04:49: when you're trying to
type it tries to guess
00:04:49 - 00:04:51: what your search term is.
00:04:51 - 00:04:53: This is an example of language modelling,
00:04:53 - 00:04:56: we'll hear a lot about language
modelling in this talk.
00:04:56 - 00:04:59: So basically we're making predictions
00:04:59 - 00:05:01: of what the continuations are going to be.
00:05:02 - 00:05:04: So what I'm telling you
00:05:04 - 00:05:07: is that generative AI is not that new.
00:05:07 - 00:05:11: So the question is, what
is the fuss, what happened?
00:05:12 - 00:05:14: So in 2023, OpenAI,
00:05:15 - 00:05:18: which is a company in California,
00:05:18 - 00:05:19: in fact, in San Francisco.
00:05:19 - 00:05:20: If you go to San Francisco,
00:05:20 - 00:05:24: you can even see the lights
at night of their building.
00:05:24 - 00:05:27: It announced GPT-4
00:05:27 - 00:05:32: and it claimed that it can
beat 90% of humans on the SAT.
00:05:33 - 00:05:34: For those of you who don't know,
00:05:34 - 00:05:37: SAT is a standardised test
00:05:37 - 00:05:40: that American school children have to take
00:05:40 - 00:05:41: to enter university,
00:05:41 - 00:05:42: it's an admissions test,
00:05:42 - 00:05:46: and it's multiple choice and
it's considered not so easy.
00:05:46 - 00:05:49: So GPT-4 can do it.
00:05:49 - 00:05:53: They also claimed that it
can get top marks in law,
00:05:53 - 00:05:55: medical exams and other exams,
00:05:55 - 00:05:59: they have a whole suite
of things that they claim,
00:05:59 - 00:06:02: well, not they claim, they
show that GPT-4 can do it.
00:06:03 - 00:06:07: Okay, aside from that, it can pass exams,
00:06:07 - 00:06:09: we can ask it to do other things.
00:06:09 - 00:06:14: So you can ask it to write text for you.
00:06:14 - 00:06:17: For example, you can have a prompt,
00:06:17 - 00:06:20: this little thing that you
see up there, it's a prompt.
00:06:20 - 00:06:23: It's what the human wants
the tool to do for them.
00:06:23 - 00:06:25: And a potential prompt could be,
00:06:25 - 00:06:27: "I'm writing an essay
00:06:27 - 00:06:29: about the use of mobile
phones during driving.
00:06:29 - 00:06:32: Can you gimme three arguments in favour?"
00:06:32 - 00:06:34: This is quite sophisticated.
00:06:34 - 00:06:35: If you asked me,
00:06:35 - 00:06:38: I'm not sure I can come
up with three arguments.
00:06:38 - 00:06:38: You can also do,
00:06:38 - 00:06:42: and these are real prompts
that actually the tool can do.
00:06:42 - 00:06:45: You tell ChatGPT or GPT in general,
00:06:45 - 00:06:47: "Act as a JavaScript developer.
00:06:47 - 00:06:50: Write a programme that checks
the information on a form.
00:06:50 - 00:06:53: Name and email are required,
but address and age are not."
00:06:53 - 00:06:55: So I'm just writing this
00:06:55 - 00:06:58: and the tool will spit out a programme.
00:06:58 - 00:07:00: And this is the best one.
00:07:00 - 00:07:03: "Create an About Me page for a website.
00:07:03 - 00:07:07: I like rock climbing, outdoor
sports, and I like to programme.
00:07:07 - 00:07:10: I started my career as a quality
engineer in the industry,
00:07:10 - 00:07:11: blah, blah, blah."
00:07:11 - 00:07:16: So I give this version of
what I want the website to be
00:07:16 - 00:07:18: and it will create it for me.
00:07:19 - 00:07:24: So, you see, we've gone from
Google Translate and Siri
00:07:24 - 00:07:25: and the auto-completion
00:07:25 - 00:07:27: to something which is a
lot more sophisticated
00:07:27 - 00:07:29: and can do a lot more things.
00:07:31 - 00:07:33: Another fun fact.
00:07:33 - 00:07:36: So this is a graph that shows
00:07:36 - 00:07:40: the time it took for ChatGPT
00:07:40 - 00:07:43: to reach 100 million users
00:07:43 - 00:07:45: compared with other tools
00:07:45 - 00:07:47: that have been launched in the past.
00:07:47 - 00:07:50: And you see our beloved Google Translate,
00:07:50 - 00:07:53: it took 78 months
00:07:53 - 00:07:56: to reach 100 million users,
00:07:56 - 00:07:57: a long time.
00:07:58 - 00:08:02: TikTok took nine months and ChatGPT, two.
00:08:03 - 00:08:08: So within two months they
had 100 million users
00:08:08 - 00:08:13: and these users pay a little
bit to use the system,
00:08:13 - 00:08:15: so you can do the multiplication
00:08:15 - 00:08:17: and figure out how much money they make.
00:08:17 - 00:08:22: Okay, so this is the history part.
00:08:22 - 00:08:27: So how did we make ChatGPT?
00:08:28 - 00:08:30: What is the technology behind this?
00:08:30 - 00:08:33: The technology it turns
out is not extremely new
00:08:33 - 00:08:35: or extremely innovative
00:08:35 - 00:08:37: or extremely difficult to comprehend.
00:08:39 - 00:08:40: So we'll talk about that today now.
00:08:42 - 00:08:45: So we'll address three questions.
00:08:45 - 00:08:48: First of all, how did we get
from the single-purpose systems
00:08:48 - 00:08:51: like Google Translate to ChatGPT,
00:08:51 - 00:08:54: which is more sophisticated
and does a lot more things?
00:08:54 - 00:08:55: And in particular,
00:08:55 - 00:08:58: what is the core technology behind ChatGPT
00:08:58 - 00:09:01: and what are the risks, if there are any?
00:09:01 - 00:09:03: And finally, I will just show you
00:09:03 - 00:09:07: a little glimpse of the future
and how it's gonna look like
00:09:07 - 00:09:09: and whether we should be worried or not
00:09:09 - 00:09:13: and you know, I won't leave you hanging,
00:09:13 - 00:09:15: please don't worry, okay?
00:09:17 - 00:09:22: Right, so, all this GPT model variants,
00:09:22 - 00:09:24: and there is a cottage industry out there,
00:09:24 - 00:09:29: I'm just using GPT as an
example because the public knows
00:09:29 - 00:09:32: and there have been a lot of, you know,
00:09:32 - 00:09:33: news articles about it,
00:09:33 - 00:09:34: but there's other models,
00:09:34 - 00:09:38: other variants of models
that we use in academia.
00:09:38 - 00:09:40: And they all work on the same principle,
00:09:40 - 00:09:43: and this principle is
called language modelling.
00:09:43 - 00:09:45: What does language modelling do?
00:09:45 - 00:09:49: It assumes we have a sequence of words.
00:09:49 - 00:09:51: The context so far.
00:09:51 - 00:09:53: And we saw this context in the completion,
00:09:53 - 00:09:55: and I have an example here.
00:09:55 - 00:10:00: Assuming my context is
the phrase "I want to,"
00:10:01 - 00:10:05: the language modelling tool
will predict what comes next.
00:10:05 - 00:10:07: So if I tell you "I want to,"
00:10:07 - 00:10:09: there is several predictions.
00:10:09 - 00:10:11: I want to shovel, I want to play,
00:10:11 - 00:10:13: I want to swim, I want to eat.
00:10:13 - 00:10:15: And depending on what we choose,
00:10:15 - 00:10:18: whether it's shovel or play or swim,
00:10:18 - 00:10:20: there is more continuations.
00:10:20 - 00:10:24: So for shovel, it will be snow,
00:10:24 - 00:10:26: for play, it can be tennis or video,
00:10:26 - 00:10:29: swim doesn't have a continuation,
00:10:29 - 00:10:31: and for eat, it will be lots and fruit.
00:10:31 - 00:10:33: Now this is a toy example,
00:10:33 - 00:10:37: but imagine now that the
computer has seen a lot of text
00:10:37 - 00:10:42: and it knows what words
follow which other words.
00:10:43 - 00:10:46: We used to count these things.
00:10:46 - 00:10:49: So I would go, I would
download a lot of data
00:10:49 - 00:10:52: and I would count, "I want to show them,"
00:10:52 - 00:10:53: how many times does it appear
00:10:53 - 00:10:55: and what are the continuations?
00:10:55 - 00:10:57: And we would have counts of these things.
00:10:57 - 00:11:00: And all of this has gone
out of the window right now
00:11:00 - 00:11:04: and we use neural networks that
don't exactly count things,
00:11:04 - 00:11:09: but predict, learn things
in a more sophisticated way,
00:11:09 - 00:11:12: and I'll show you in a
moment how it's done.
00:11:12 - 00:11:17: So ChatGPT and GPT variants
00:11:17 - 00:11:19: are based on this principle
00:11:19 - 00:11:23: of I have some context, I
will predict what comes next.
00:11:23 - 00:11:25: And that's the prompt,
00:11:25 - 00:11:28: the prompt that I gave
you, these things here,
00:11:28 - 00:11:29: these are prompts,
00:11:29 - 00:11:31: this is the context,
00:11:31 - 00:11:33: and then it needs to do the task.
00:11:33 - 00:11:35: What would come next?
00:11:35 - 00:11:37: In some cases it would
be the three arguments.
00:11:37 - 00:11:41: In the case of the web
developer, it would be a webpage.
00:11:42 - 00:11:47: Okay, the task of language
modelling is we have the context,
00:11:47 - 00:11:48: and this changed the example now.
00:11:48 - 00:11:51: It says "The colour of the sky is."
00:11:51 - 00:11:54: And we have a neural language model,
00:11:54 - 00:11:57: this is just an algorithm,
00:11:57 - 00:12:02: that will predict what is
the most likely continuation,
00:12:03 - 00:12:04: and likelihood matters.
00:12:05 - 00:12:09: These are all predicated
on actually making guesses
00:12:09 - 00:12:11: about what's gonna come next.
00:12:11 - 00:12:13: And that's why sometimes they fail,
00:12:13 - 00:12:15: because they predict
the most likely answer
00:12:15 - 00:12:18: whereas you want a less likely one.
00:12:18 - 00:12:19: But this is how they're trained,
00:12:19 - 00:12:22: they're trained to come up
with what is most likely.
00:12:22 - 00:12:25: Okay, so we don't count these things,
00:12:25 - 00:12:28: we try to predict them
using this language model.
00:12:29 - 00:12:34: So how would you build
your own language model?
00:12:34 - 00:12:37: This is a recipe, this is
how everybody does this.
00:12:37 - 00:12:41: So, step one, we need a lot of data.
00:12:41 - 00:12:45: We need to collect a ginormous corpus.
00:12:45 - 00:12:47: So these are words.
00:12:47 - 00:12:50: And where will we find
such a ginormous corpus?
00:12:50 - 00:12:52: I mean, we go to the web, right?
00:12:52 - 00:12:56: And we download the whole of Wikipedia,
00:12:56 - 00:12:58: Stack Overflow pages,
00:12:58 - 00:13:01: Quora, social media, GitHub, Reddit,
00:13:01 - 00:13:03: whatever you can find out there.
00:13:03 - 00:13:06: I mean, work out the
permissions, it has to be legal.
00:13:06 - 00:13:08: You download all this corpus.
00:13:09 - 00:13:10: And then what do you do?
00:13:10 - 00:13:11: Then you have this language model.
00:13:11 - 00:13:14: I haven't told you what
exactly this language model is,
00:13:14 - 00:13:15: there is an example,
00:13:15 - 00:13:17: and I haven't told you
what the neural network
00:13:17 - 00:13:18: that does the prediction is,
00:13:18 - 00:13:20: but assuming you have it.
00:13:20 - 00:13:22: So you have this machinery
00:13:22 - 00:13:24: that will do the learning for you
00:13:24 - 00:13:28: and the task now is to
predict the next word,
00:13:28 - 00:13:30: but how do we do it?
00:13:30 - 00:13:32: And this is the genius part.
00:13:33 - 00:13:36: We have the sentences in the corpus.
00:13:36 - 00:13:38: We can remove some of them
00:13:38 - 00:13:40: and we can have the language model
00:13:40 - 00:13:43: predict the sentences we have removed.
00:13:43 - 00:13:45: This is dead cheap.
00:13:46 - 00:13:47: I just remove things,
00:13:47 - 00:13:49: I pretend they're not there,
00:13:49 - 00:13:52: and I get the language
model to predict them.
00:13:52 - 00:13:55: So I will randomly truncate,
00:13:55 - 00:13:56: truncate means remove,
00:13:56 - 00:13:59: the last part of the input sentence.
00:13:59 - 00:14:01: I will calculate with this neural network
00:14:01 - 00:14:04: the probability of the missing words.
00:14:04 - 00:14:05: If I get it right, I'm good.
00:14:05 - 00:14:06: If I'm not right,
00:14:06 - 00:14:09: I have to go back and
re-estimate some things
00:14:09 - 00:14:11: because obviously I made a mistake,
00:14:11 - 00:14:12: and I keep going.
00:14:12 - 00:14:14: I will adjust and feedback to the model
00:14:14 - 00:14:16: and then I will compare
what the model predicted
00:14:16 - 00:14:17: to the ground truth
00:14:17 - 00:14:19: because I've removed the
words in the first place
00:14:19 - 00:14:22: so I actually know what the real truth is.
00:14:22 - 00:14:24: And we keep going
00:14:24 - 00:14:28: for some months or maybe years.
00:14:28 - 00:14:30: No, months, let's say.
00:14:30 - 00:14:32: So it will take some
time to do this process
00:14:32 - 00:14:33: because as you can appreciate
00:14:33 - 00:14:36: I have a very large corpus
and I have many sentences
00:14:36 - 00:14:38: and I have to do the prediction
00:14:38 - 00:14:42: and then go back and correct
my mistake and so on.
00:14:42 - 00:14:43: But in the end,
00:14:43 - 00:14:45: the thing will converge
and I will get my answer.
00:14:46 - 00:14:50: So the tool in the middle that I've shown,
00:14:50 - 00:14:54: this tool here, this language model,
00:14:54 - 00:14:58: a very simple language
model looks a bit like this.
00:14:58 - 00:15:01: And maybe the audience has seen these,
00:15:01 - 00:15:04: this is a very naive graph,
00:15:04 - 00:15:07: but it helps to illustrate
the point of what it does.
00:15:07 - 00:15:12: So this neural network language
model will have some input
00:15:12 - 00:15:16: which is these nodes in
the, as we look at it,
00:15:16 - 00:15:18: well, my right and your right, okay.
00:15:18 - 00:15:23: So the nodes here on
the right are the input
00:15:23 - 00:15:27: and the nodes at the
very left are the output.
00:15:27 - 00:15:31: So we will present this neural
network with five inputs,
00:15:34 - 00:15:36: the five circles,
00:15:36 - 00:15:38: and we have three outputs,
00:15:38 - 00:15:39: the three circles.
00:15:39 - 00:15:41: And there is stuff in the middle
00:15:41 - 00:15:43: that I didn't say anything about.
00:15:43 - 00:15:45: These are layers.
00:15:45 - 00:15:47: These are more nodes
00:15:47 - 00:15:51: that are supposed to be
abstractions of my input.
00:15:51 - 00:15:52: So they generalise.
00:15:52 - 00:15:57: The idea is if I put more
layers on top of layers,
00:15:57 - 00:16:00: the middle layers will
generalise the input
00:16:00 - 00:16:04: and will be able to see
patterns that are not there.
00:16:04 - 00:16:05: So you have these nodes
00:16:05 - 00:16:08: and the input to the nodes
are not exactly words,
00:16:08 - 00:16:11: they're vectors, so series of numbers,
00:16:11 - 00:16:13: but forget that for now.
00:16:13 - 00:16:16: So we have some input, we have
some layers in the middle,
00:16:16 - 00:16:17: we have some output.
00:16:17 - 00:16:20: And this now has these
connections, these edges,
00:16:20 - 00:16:22: which are the weights,
00:16:22 - 00:16:25: this is what the network will learn.
00:16:25 - 00:16:27: And these weights are basically numbers,
00:16:27 - 00:16:30: and here it's all fully connected,
00:16:30 - 00:16:32: so I have very many connections.
00:16:32 - 00:16:35: Why am I going through this process
00:16:35 - 00:16:37: of actually telling you all of that?
00:16:37 - 00:16:38: You will see in a minute.
00:16:38 - 00:16:42: So you can work out
00:16:42 - 00:16:46: how big or how small
this neural network is
00:16:46 - 00:16:51: depending on the numbers
of connections it has.
00:16:51 - 00:16:54: So for this toy neural
network we have here,
00:16:54 - 00:16:58: I have worked out the number of weights,
00:16:58 - 00:17:01: we call them also parameters,
00:17:01 - 00:17:02: that this neural network has
00:17:02 - 00:17:05: and that the model needs to learn.
00:17:05 - 00:17:09: So the parameters are the
number of units as input,
00:17:09 - 00:17:12: in this case it's 5,
00:17:12 - 00:17:16: times the units in the next layer, 8.
00:17:16 - 00:17:19: Plus 8, this plus 8 is a bias,
00:17:19 - 00:17:23: it's a cheating thing that
these neural networks have.
00:17:23 - 00:17:25: Again, you need to learn it
00:17:25 - 00:17:28: and it sort of corrects a
little bit the neural network
00:17:28 - 00:17:29: if it's off.
00:17:29 - 00:17:30: It's actually genius.
00:17:30 - 00:17:32: If the prediction is not right,
00:17:32 - 00:17:33: it tries to correct it a little bit.
00:17:33 - 00:17:35: So for the purposes of this talk,
00:17:35 - 00:17:38: I'm not going to go into the details,
00:17:38 - 00:17:39: all I want you to see
00:17:39 - 00:17:41: is that there is a way of
working out the parameters,
00:17:41 - 00:17:45: which is basically the
number of input units
00:17:45 - 00:17:49: times the units my input is going to,
00:17:49 - 00:17:51: and for this fully connected network,
00:17:51 - 00:17:53: if we add up everything,
00:17:53 - 00:17:58: we come up with 99
trainable parameters, 99.
00:17:58 - 00:18:02: This is a small network
for all purposes, right?
00:18:02 - 00:18:03: But I want you to remember this,
00:18:03 - 00:18:05: this small network is 99 parameters.
00:18:05 - 00:18:10: When you hear this network
is a billion parameters,
00:18:10 - 00:18:14: I want you to imagine how
big this will be, okay?
00:18:14 - 00:18:17: So 99 only for this toy neural network.
00:18:17 - 00:18:21: And this is how we judge
how big the model is,
00:18:21 - 00:18:24: how long it took and how much it cost,
00:18:24 - 00:18:26: it's the number of parameters.
00:18:27 - 00:18:29: In reality, in reality, though,
00:18:29 - 00:18:31: no one is using this network.
00:18:31 - 00:18:33: Maybe in my class,
00:18:33 - 00:18:36: if I have a first year undergraduate class
00:18:36 - 00:18:37: and I introduce neural networks,
00:18:37 - 00:18:39: I will use this as an example.
00:18:39 - 00:18:42: In reality, what people
use is these monsters
00:18:42 - 00:18:47: that are made of blocks,
00:18:47 - 00:18:52: and what block means they're
made of other neural networks.
00:18:52 - 00:18:57: So I don't know how many people
have heard of transformers.
00:18:57 - 00:18:57: I hope no one.
00:18:57 - 00:18:59: Oh wow, okay.
00:18:59 - 00:19:03: So transformers are these neural networks
00:19:03 - 00:19:06: that we use to build ChatGPT.
00:19:06 - 00:19:09: And in fact GPT stands for
00:19:09 - 00:19:12: generative pre-trained transformers.
00:19:12 - 00:19:15: So transformer is even in the title.
00:19:15 - 00:19:19: So this is a sketch of a transformer.
00:19:19 - 00:19:21: So you have your input
00:19:21 - 00:19:24: and the input is not words, like I said,
00:19:24 - 00:19:25: here it says embeddings,
00:19:25 - 00:19:28: embeddings is another word for vectors.
00:19:28 - 00:19:32: And then you will have this,
00:19:32 - 00:19:34: a bigger version of this network,
00:19:34 - 00:19:38: multiplied into these blocks.
00:19:38 - 00:19:42: And each block is this complicated system
00:19:42 - 00:19:46: that has some neural networks inside it.
00:19:46 - 00:19:48: We're not gonna go into
the detail, I don't want,
00:19:48 - 00:19:50: I please don't go,
00:19:50 - 00:19:51: all I'm trying,
(audience laughs)
00:19:51 - 00:19:55: all I'm trying to say is that, you know,
00:19:55 - 00:20:00: we have these blocks stacked
on top of each other,
00:20:00 - 00:20:02: the transformer has eight of those,
00:20:02 - 00:20:04: which are mini neural networks,
00:20:04 - 00:20:06: and this task remains the same.
00:20:06 - 00:20:08: That's what I want you
to take out of this.
00:20:08 - 00:20:12: Input goes in the context,
"the chicken walked,"
00:20:12 - 00:20:13: we're doing some processing,
00:20:13 - 00:20:17: and our task is to
predict the continuation,
00:20:17 - 00:20:18: which is "across the road."
00:20:18 - 00:20:21: And this EOS means end of sentence
00:20:21 - 00:20:23: because we need to tell the neural network
00:20:23 - 00:20:24: that our sentence finished.
00:20:24 - 00:20:26: I mean they're kind of dumb, right?
00:20:26 - 00:20:27: We need to tell them everything.
00:20:27 - 00:20:30: When I hear like AI will take
over the world, I go like,
00:20:30 - 00:20:33: Really? We have to actually spell it out.
00:20:33 - 00:20:37: Okay, so, this is the transformer,
00:20:37 - 00:20:38: the king of architectures,
00:20:38 - 00:20:40: the transformers came in 2017.
00:20:42 - 00:20:45: Nobody's working on new
architectures right now.
00:20:45 - 00:20:48: It is a bit sad, like
everybody's using these things.
00:20:48 - 00:20:50: They used to be like some
pluralism but now no,
00:20:50 - 00:20:54: everybody's using transformers,
we've decided they're great.
00:20:54 - 00:20:58: Okay, so, what we're gonna do with this,
00:20:58 - 00:21:01: and this is kind of important
and the amazing thing,
00:21:01 - 00:21:03: is we're gonna do
self-supervised learning.
00:21:03 - 00:21:04: And this is what I said,
00:21:04 - 00:21:08: we have the sentence,
we truncate, we predict,
00:21:08 - 00:21:12: and we keep going till we
learn these probabilities.
00:21:12 - 00:21:15: Okay? You're with me so far?
00:21:15 - 00:21:18: Good, okay, so,
00:21:18 - 00:21:21: once we have our transformer
00:21:21 - 00:21:26: and we've given it all this
data that there is in the world,
00:21:26 - 00:21:28: then we have a pre-trained model.
00:21:28 - 00:21:30: That's why GPT is called
00:21:30 - 00:21:32: the generative pre-trained transformer.
00:21:32 - 00:21:35: This is a baseline model that we have
00:21:35 - 00:21:39: and has seen a lot of
things about the world
00:21:39 - 00:21:40: in the form of text.
00:21:40 - 00:21:42: And then what we normally do,
00:21:42 - 00:21:44: we have this general purpose model
00:21:44 - 00:21:46: and we need to specialise it somehow
00:21:46 - 00:21:48: for a specific task.
00:21:48 - 00:21:50: And this is what is called fine-tuning.
00:21:50 - 00:21:54: So that means that the
network has some weights
00:21:54 - 00:21:57: and we have to specialise the network.
00:21:57 - 00:21:59: We'll take, initialise the weights
00:21:59 - 00:22:01: with what we know from the pre-training,
00:22:01 - 00:22:03: and then in the specific
task we will narrow
00:22:03 - 00:22:05: a new set of weights.
00:22:05 - 00:22:09: So for example, if I have medical data,
00:22:09 - 00:22:11: I will take my pre-trained model,
00:22:11 - 00:22:14: I will specialise it to this medical data,
00:22:14 - 00:22:18: and then I can do something
that is specific for this task,
00:22:18 - 00:22:22: which is, for example, write
a diagnosis from a report.
00:22:22 - 00:22:27: Okay, so this notion of
fine-tuning is very important
00:22:27 - 00:22:31: because it allows us to do
special-purpose applications
00:22:31 - 00:22:33: for these generic pre-trained models.
00:22:35 - 00:22:37: Now, and people think that
GPT and all of these things
00:22:37 - 00:22:39: are general purpose,
00:22:39 - 00:22:42: but they are fine-tuned
to be general purpose
00:22:42 - 00:22:43: and we'll see how.
00:22:45 - 00:22:49: Okay, so, here's the question now.
00:22:49 - 00:22:52: We have this basic technology
to do this pre-training
00:22:52 - 00:22:56: and I told you how to do it,
if you download all of the web.
00:22:56 - 00:22:59: How good can a language
model become, right?
00:22:59 - 00:23:01: How does it become great?
00:23:01 - 00:23:06: Because when GPT came
out in GPT-1 and GPT-2,
00:23:06 - 00:23:07: they were not amazing.
00:23:09 - 00:23:11: So the bigger, the better.
00:23:13 - 00:23:15: Size is all that matters, I'm afraid.
00:23:15 - 00:23:18: This is very bad because
we used to, you know,
00:23:18 - 00:23:19: people didn't believe in scale
00:23:19 - 00:23:22: and now we see that
scale is very important.
00:23:22 - 00:23:25: So, since 2018,
00:23:25 - 00:23:30: we've witnessed an
absolutely extreme increase
00:23:32 - 00:23:34: in model sizes.
00:23:34 - 00:23:36: And I have some graphs to show this.
00:23:36 - 00:23:39: Okay, I hope people at the
back can see this graph.
00:23:39 - 00:23:40: Yeah, you should be all right.
00:23:40 - 00:23:45: So this graph shows
00:23:45 - 00:23:47: the number of parameters.
00:23:47 - 00:23:50: Remember, the toy neural network had 99.
00:23:50 - 00:23:54: The number of parameters
that these models have.
00:23:54 - 00:23:57: And we start with a normal amount.
00:23:57 - 00:23:58: Well, normal for GPT-1.
00:23:58 - 00:24:01: And we go up to GPT-4,
00:24:01 - 00:24:06: which has one trillion parameters.
00:24:07 - 00:24:10: Huge, one trillion.
00:24:10 - 00:24:12: This is a very, very, very big model.
00:24:12 - 00:24:16: And you can see here the
ant brain and the rat brain
00:24:16 - 00:24:19: and we go up to the human brain.
00:24:19 - 00:24:21: The human brain has,
00:24:23 - 00:24:24: not a trillion,
00:24:24 - 00:24:27: 100 trillion parameters.
00:24:27 - 00:24:30: So we are a bit off,
00:24:30 - 00:24:32: we're not at the human brain level yet
00:24:32 - 00:24:34: and maybe we'll never get there
00:24:34 - 00:24:37: and we can't compare
GPT to the human brain
00:24:37 - 00:24:41: but I'm just giving you an
idea of how big this model is.
00:24:42 - 00:24:46: Now what about the words it's seen?
00:24:46 - 00:24:48: So this graph shows us the number of words
00:24:48 - 00:24:52: processed by these language
models during their training
00:24:52 - 00:24:55: and you will see that
there has been an increase,
00:24:55 - 00:25:00: but the increase has not been
as big as the parameters.
00:25:00 - 00:25:04: So the community started focusing
00:25:04 - 00:25:06: on the parameter size of these models,
00:25:06 - 00:25:08: whereas in fact we now know
00:25:08 - 00:25:11: that it needs to see
a lot of text as well.
00:25:11 - 00:25:16: So GPT-4 has seen approximately,
00:25:16 - 00:25:18: I don't know, a few billion words.
00:25:19 - 00:25:24: All the human written text
is I think 100 billion,
00:25:24 - 00:25:28: so it's sort of approaching this.
00:25:28 - 00:25:32: You can also see what a human
reads in their lifetime,
00:25:32 - 00:25:34: it's a lot less.
00:25:34 - 00:25:35: Even if they read, you know,
00:25:35 - 00:25:37: because people nowadays, you know,
00:25:37 - 00:25:39: they read but they don't read fiction,
00:25:39 - 00:25:41: they read the phone, anyway.
00:25:41 - 00:25:42: You see the English Wikipedia,
00:25:42 - 00:25:46: so we are approaching the level of
00:25:46 - 00:25:49: the text that is out
there that we can get.
00:25:49 - 00:25:52: And in fact, one may
say, well, GPT is great,
00:25:52 - 00:25:54: you can actually use it
to generate more text
00:25:54 - 00:25:56: and then use this text
that GPT has generated
00:25:56 - 00:25:58: and then retrain the model.
00:25:58 - 00:26:00: But we know this text is not exactly right
00:26:00 - 00:26:03: and in fact it's diminished returns,
00:26:03 - 00:26:04: so we're gonna plateau at some point.
00:26:06 - 00:26:08: Okay, how much does it cost?
00:26:10 - 00:26:15: Now, okay, so GPT-4 cost
00:26:16 - 00:26:21: $100 million, okay?
00:26:21 - 00:26:25: So when should they start doing it again?
00:26:25 - 00:26:28: So obviously this is not
a process you have to do
00:26:28 - 00:26:29: over and over again.
00:26:29 - 00:26:31: You have to think very well
00:26:31 - 00:26:34: and you make a mistake and
you lost like $50 million.
00:26:38 - 00:26:41: You can't start again so you
have to be very sophisticated
00:26:41 - 00:26:43: as to how you engineer the training
00:26:43 - 00:26:47: because a mistake costs money.
00:26:47 - 00:26:48: And of course not everybody can do this,
00:26:48 - 00:26:51: not everybody has $100 million.
00:26:51 - 00:26:54: They can do it because they
have Microsoft backing them,
00:26:54 - 00:26:56: not everybody, okay.
00:26:58 - 00:27:01: Now this is a video that is
supposed to play and illustrate,
00:27:01 - 00:27:03: let's see if it will work,
00:27:03 - 00:27:06: the effects of scaling, okay.
00:27:06 - 00:27:08: So I will play it one more.
00:27:09 - 00:27:12: So these are tasks that you can do
00:27:12 - 00:27:14: and it's the number of tasks
00:27:15 - 00:27:18: against the number of parameters.
00:27:18 - 00:27:20: So we start with 8 billion parameters
00:27:20 - 00:27:22: and we can do a few tasks.
00:27:23 - 00:27:26: And then the tasks
increase, so summarization,
00:27:27 - 00:27:30: question answering, translation.
00:27:30 - 00:27:35: And once we move to
540 billion parameters,
00:27:35 - 00:27:36: we have more tasks.
00:27:36 - 00:27:38: We start with very simple ones,
00:27:39 - 00:27:41: like code completion.
00:27:42 - 00:27:45: And then we can do reading comprehension
00:27:45 - 00:27:47: and language understanding
and translation.
00:27:47 - 00:27:51: So you get the picture,
the tree flourishes.
00:27:51 - 00:27:54: So this is what people
discovered with scaling.
00:27:54 - 00:27:57: If you scale the language
model, you can do more tasks.
00:27:58 - 00:28:02: Okay, so now.
00:28:04 - 00:28:06: Maybe we are done.
00:28:07 - 00:28:12: But what people discovered
is if you actually take GPT
00:28:12 - 00:28:14: and you put it out there,
00:28:14 - 00:28:18: it actually doesn't behave
like people want it to behave
00:28:18 - 00:28:21: because this is a language
model trained to predict
00:28:21 - 00:28:22: and complete sentences
00:28:22 - 00:28:26: and humans want to use
GPT for other things
00:28:26 - 00:28:29: because they have their own tasks
00:28:29 - 00:28:31: that the developers hadn't thought of.
00:28:31 - 00:28:35: So then the notion of
fine-tuning comes in,
00:28:35 - 00:28:37: it never left us.
00:28:37 - 00:28:39: So now what we're gonna do
00:28:39 - 00:28:42: is we're gonna collect
a lot of instructions.
00:28:42 - 00:28:44: So instructions are examples
00:28:44 - 00:28:47: of what people want
ChatGPT to do for them,
00:28:47 - 00:28:50: such as answer the following question,
00:28:50 - 00:28:54: or answer the question step by step.
00:28:54 - 00:28:58: And so we're gonna give these
demonstrations to the model,
00:28:58 - 00:29:03: and in fact, almost
2,000 of such examples,
00:29:03 - 00:29:05: and we're gonna fine-tune.
00:29:05 - 00:29:07: So we're gonna tell this language model,
00:29:07 - 00:29:09: look, these are the
tasks that people want,
00:29:09 - 00:29:11: try to learn them.
00:29:12 - 00:29:14: And then an interesting thing happens,
00:29:14 - 00:29:17: is that we can actually then generalise
00:29:17 - 00:29:20: to unseen tasks, unseen instructions,
00:29:20 - 00:29:23: because you and I may have
different usage purposes
00:29:23 - 00:29:25: for these language models.
00:29:27 - 00:29:30: Okay, but here's the problem.
00:29:33 - 00:29:34: We have an alignment problem
00:29:34 - 00:29:36: and this is actually very important
00:29:36 - 00:29:41: and something that will not
leave us for the future.
00:29:42 - 00:29:43: And the question is,
00:29:43 - 00:29:45: how do we create an agent
00:29:45 - 00:29:49: that behaves in accordance
with what a human wants?
00:29:49 - 00:29:53: And I know there's many
words and questions here.
00:29:53 - 00:29:54: But the real question is,
00:29:54 - 00:29:57: if we have AI systems with skills
00:29:57 - 00:30:00: that we find important or useful,
00:30:00 - 00:30:04: how do we adapt those systems
to reliably use those skills
00:30:04 - 00:30:06: to do the things we want?
00:30:08 - 00:30:09: And there is a framework
00:30:09 - 00:30:14: that is called the HHH
framing of the problem.
00:30:15 - 00:30:20: So we want GPT to be helpful,
honest, and harmless.
00:30:21 - 00:30:24: And this is the bare minimum.
00:30:24 - 00:30:26: So what does it mean, helpful?
00:30:26 - 00:30:28: It it should follow instructions
00:30:28 - 00:30:31: and perform the tasks
we want it to perform
00:30:31 - 00:30:33: and provide answers for them
00:30:33 - 00:30:35: and ask relevant questions
00:30:35 - 00:30:40: according to the user intent, and clarify.
00:30:40 - 00:30:41: So if you've been following,
00:30:41 - 00:30:43: in the beginning, GPT did none of this,
00:30:43 - 00:30:45: but slowly it became better
00:30:45 - 00:30:50: and it now actually asks for
these clarification questions.
00:30:50 - 00:30:51: It should be accurate,
00:30:51 - 00:30:55: something that is not
100% there even to this,
00:30:55 - 00:30:58: there is, you know,
inaccurate information.
00:30:58 - 00:31:03: And avoid toxic, biassed,
or offensive responses.
00:31:03 - 00:31:06: And now here's a question I have for you.
00:31:06 - 00:31:09: How will we get the model
to do all of these things?
00:31:12 - 00:31:16: You know the answer. Fine-tuning.
00:31:17 - 00:31:20: Except that we're gonna do
a different fine-tuning.
00:31:20 - 00:31:25: We're gonna ask the humans to
do some preferences for us.
00:31:25 - 00:31:28: So in terms of helpful, we're gonna ask,
00:31:28 - 00:31:32: an example is, "What causes
the seasons to change?"
00:31:32 - 00:31:35: And then we'll give two
options to the human.
00:31:35 - 00:31:36: "Changes occur all the time
00:31:36 - 00:31:39: and it's an important
aspect of life," bad.
00:31:39 - 00:31:41: "The seasons are caused primarily
00:31:41 - 00:31:44: by the tilt of the Earth's axis," good.
00:31:44 - 00:31:46: So we'll get this preference course
00:31:46 - 00:31:49: and then we'll train the model again
00:31:49 - 00:31:51: and then it will know.
00:31:51 - 00:31:53: So fine-tuning is very important.
00:31:53 - 00:31:56: And now, it was expensive as it was,
00:31:56 - 00:31:58: now we make it even more expensive
00:31:58 - 00:32:00: because we add a human
into the mix, right?
00:32:00 - 00:32:02: Because we have to pay these humans
00:32:02 - 00:32:03: that give us the preferences,
00:32:03 - 00:32:05: we have to think of the tasks.
00:32:05 - 00:32:07: The same for honesty.
00:32:07 - 00:32:09: "Is it possible to
prove that P equals NP?"
00:32:09 - 00:32:12: "No, it's impossible," is
not great as an answer.
00:32:12 - 00:32:15: "That is considered a very
difficult and unsolved problem
00:32:15 - 00:32:17: in computer science," it's better.
00:32:17 - 00:32:20: And we have similar for harmless.
00:32:20 - 00:32:22: Okay, so I think it's time,
00:32:22 - 00:32:24: let's see if we'll do a demo.
00:32:24 - 00:32:27: Yeah, that's bad if you
remove all the files.
00:32:28 - 00:32:30: Okay, hold on, okay.
00:32:30 - 00:32:33: So now we have GPT here.
00:32:33 - 00:32:35: I'll do some questions
00:32:35 - 00:32:38: and then we'll take some
questions from the audience, okay?
00:32:38 - 00:32:40: So let's ask one question.
00:32:40 - 00:32:43: "Is the UK a monarchy?"
00:32:43 - 00:32:45: Can you see it up there? I'm not sure.
00:32:48 - 00:32:50: And it's not generating.
00:32:53 - 00:32:55: Oh, perfect, okay.
00:32:55 - 00:32:56: So what do you observe?
00:32:56 - 00:32:58: First thing, too long.
00:32:58 - 00:33:00: I always have this beef with this.
00:33:00 - 00:33:02: It's too long.
(audience laughs)
00:33:02 - 00:33:03: You see what it says?
00:33:03 - 00:33:08: "As of my last knowledge
update in September 2021,
00:33:08 - 00:33:10: the United Kingdom is a
constitutional monarchy."
00:33:10 - 00:33:12: It could be that it wasn't anymore, right?
00:33:12 - 00:33:13: Something happened.
00:33:13 - 00:33:16: "This means that while there is a monarch,
00:33:16 - 00:33:18: the reigning monarch as to that time
00:33:18 - 00:33:21: was Queen Elizabeth III."
00:33:21 - 00:33:22: So it tells you, you know,
00:33:22 - 00:33:23: I don't know what happened,
00:33:23 - 00:33:26: at that time there was a Queen Elizabeth.
00:33:26 - 00:33:31: Now if you ask it, who,
sorry, "Who is Rishi?
00:33:32 - 00:33:36: If I could type, "Rishi Sunak,"
00:33:36 - 00:33:37: does it know?
00:33:45 - 00:33:46: "A British politician.
00:33:46 - 00:33:48: As my last knowledge update,
00:33:48 - 00:33:50: he was the Chancellor of the Exchequer."
00:33:50 - 00:33:54: So it does not know that
he's the Prime Minister.
00:33:55 - 00:33:57: "Write me a poem,
00:33:57 - 00:34:01: write me a poem about."
00:34:02 - 00:34:04: What do we want it to be about?
00:34:04 - 00:34:06: Give me two things, eh?
00:34:06 - 00:34:08: - [Audience Member] Generative AI.
00:34:08 - 00:34:10: (audience laughs)
- It will know.
00:34:10 - 00:34:13: It will know, let's do
another point about...
00:34:14 - 00:34:15: - [Audience Members] Cats.
00:34:16 - 00:34:19: - A cat and a squirrel, we'll
do a cat and a squirrel.
00:34:19 - 00:34:21: "A cat and a squirrel."
00:34:27 - 00:34:29: "A cat and a squirrel, they meet and know.
00:34:29 - 00:34:31: A tale of curiosity," whoa.
00:34:31 - 00:34:33: (audience laughs)
00:34:33 - 00:34:37: Oh my god, okay, I will not read this.
00:34:37 - 00:34:41: You know, they want me to
finish at 8:00, so, right.
00:34:42 - 00:34:47: Let's say, "Can you try a shorter poem?"
00:34:47 - 00:34:49: - [Audience Member] Try a haiku.
00:34:49 - 00:34:51: - "Can you try,
00:34:52 - 00:34:54: can you try to give me a haiku?"
00:34:54 - 00:34:59: To give me a hai, I cannot type, haiku.
00:35:05 - 00:35:08: "Amidst autumn's gold, leaves
whisper secrets untold,
00:35:08 - 00:35:11: nature's story, bold."
00:35:11 - 00:35:13: (audience member claps)
Okay.
00:35:13 - 00:35:16: Don't clap, okay, let's, okay, one more.
00:35:16 - 00:35:20: So does the audience have
anything that they want,
00:35:20 - 00:35:22: but challenging, that you want to ask?
00:35:23 - 00:35:24: Yes?
00:35:24 - 00:35:27: - [Audience Member] What
school did Alan Turing go to?
00:35:27 - 00:35:30: - Perfect, "What school
00:35:30 - 00:35:35: did Alan Turing go to?"
00:35:39 - 00:35:41: Oh my God.
(audience laughs)
00:35:41 - 00:35:42: He went, do you know?
00:35:42 - 00:35:44: I don't know whether it's
true, this is the problem.
00:35:44 - 00:35:46: Sherborne School, can somebody verify?
00:35:46 - 00:35:50: King's College, Cambridge, Princeton?
00:35:50 - 00:35:52: Yes, okay, ah, here's another one.
00:35:52 - 00:35:57: "Tell me a joke about Alan Turing."
00:35:58 - 00:36:01: Okay, I cannot type but it will, okay.
00:36:01 - 00:36:02: "Light-hearted joke.
00:36:02 - 00:36:04: Why did Alan Turing
keep his computer cold?
00:36:04 - 00:36:06: Because he didn't want it to catch bytes."
00:36:10 - 00:36:11: (audience laughs)
Bad.
00:36:12 - 00:36:16: Okay, okay.
- Explain why that's funny.
00:36:16 - 00:36:19: (audience laughs)
- Ah, very good one.
00:36:19 - 00:36:24: "Why is this a funny joke?"
00:36:28 - 00:36:30: And where is it? Oh god.
00:36:30 - 00:36:31: (audience laughs)
00:36:31 - 00:36:35: Okay, "Catch bytes sounds
similar to catch colds."
00:36:35 - 00:36:37: (audience laughs)
00:36:37 - 00:36:39: "Catching bytes is a humorous
twist on this phrase,"
00:36:39 - 00:36:40: oh my God.
00:36:40 - 00:36:42: "The humour comes from the clever wordplay
00:36:42 - 00:36:44: and the unexpected."
(audience laughs)
00:36:44 - 00:36:45: Okay, you lose the will to live,
00:36:45 - 00:36:50: but it does explain, it
does explain, okay, right.
00:36:50 - 00:36:52: One last order from you guys.
00:36:52 - 00:36:54: - [Audience Member] What is consciousness?
00:36:54 - 00:36:57: - It will know because
it has seen definitions
00:36:57 - 00:37:00: and it will spit out like a huge thing.
00:37:00 - 00:37:01: Shall we try?
00:37:02 - 00:37:05: (audience talks indistinctly)
- Say again?
00:37:05 - 00:37:07: - [Audience Member] Write
a song about relativity.
00:37:07 - 00:37:10: - Okay, "Write a song."
- Short.
00:37:10 - 00:37:13: (audience laughs)
- You are learning very fast.
00:37:13 - 00:37:18: "A short song about relativity."
00:37:22 - 00:37:25: Oh goodness me.
(audience laughs)
00:37:25 - 00:37:29: (audience laughs)
00:37:29 - 00:37:33: This is short?
(audience laughs)
00:37:33 - 00:37:35: All right, outro, okay, so see,
00:37:35 - 00:37:37: it doesn't follow instructions.
00:37:37 - 00:37:38: It is not helpful.
00:37:38 - 00:37:40: And this has been fine-tuned.
00:37:40 - 00:37:42: Okay, so the best was here.
00:37:42 - 00:37:45: It had something like, where was it?
00:37:45 - 00:37:47: "Einstein said, 'Eureka!" one fateful day,
00:37:47 - 00:37:51: as he pondered the stars
in his own unique way.
00:37:51 - 00:37:54: The theory of relativity, he did unfold,
00:37:54 - 00:37:57: a cosmic story, ancient and bold."
00:37:57 - 00:37:58: I mean, kudos to that, okay.
00:37:58 - 00:38:00: Now let's go back to the talk,
00:38:02 - 00:38:05: because I want to talk a
little bit, presentation,
00:38:05 - 00:38:09: I want to talk a little
bit about, you know,
00:38:09 - 00:38:12: is it good, is it bad, is
it fair, are we in danger?
00:38:12 - 00:38:14: Okay, so it's virtually impossible
00:38:14 - 00:38:18: to regulate the content
they're exposed to, okay?
00:38:18 - 00:38:21: And there's always gonna
be historical biases.
00:38:21 - 00:38:23: We saw this with the
Queen and Rishi Sunak.
00:38:24 - 00:38:27: And they may occasionally exhibit
00:38:27 - 00:38:30: various types of undesirable behaviour.
00:38:30 - 00:38:34: For example, this is famous.
00:38:35 - 00:38:38: Google showcased the model called Bard
00:38:38 - 00:38:43: and they released this tweet
and they were asking Bard,
00:38:43 - 00:38:46: "What new discoveries from
the James Webb Space Telescope
00:38:46 - 00:38:49: can I tell my nine-year-old about?"
00:38:49 - 00:38:53: And it's spit out this
thing, three things.
00:38:53 - 00:38:54: Amongst them it said
00:38:54 - 00:38:57: that "this telescope took
the very first picture
00:38:57 - 00:39:02: of a planet outside of
our own solar system."
00:39:02 - 00:39:04: And here comes Grant Tremblay,
00:39:04 - 00:39:06: who is an astrophysicist, a serious guy,
00:39:06 - 00:39:10: and he said, "I'm really sorry,
I'm sure Bard is amazing.
00:39:10 - 00:39:13: But it did not take the first image
00:39:13 - 00:39:16: of a planet outside our solar system.
00:39:16 - 00:39:20: It was done by this other people in 2004."
00:39:20 - 00:39:23: And what happened with this
is that this error wiped
00:39:23 - 00:39:28: $100 billion out of
Google's company Alphabet.
00:39:28 - 00:39:31: Okay, bad.
00:39:32 - 00:39:35: If you ask ChatGPT, "Tell
me a joke about men,"
00:39:35 - 00:39:39: it gives you a joke and
it says it might be funny.
00:39:39 - 00:39:42: "Why do men need instant
replay on TV sports?
00:39:42 - 00:39:44: Because after 30 seconds,
they forget what happened."
00:39:44 - 00:39:46: I hope you find it amusing.
00:39:46 - 00:39:49: If you ask about women, it refuses.
00:39:49 - 00:39:52: (audience laughs)
00:39:52 - 00:39:56: Okay, yes.
00:39:56 - 00:39:58: - It's fine-tuned.
- It's fine-tuned, exactly.
00:39:58 - 00:40:00: (audience laughs)
00:40:00 - 00:40:02: "Which is the worst
dictator of this group?
00:40:02 - 00:40:05: Trump, Hitler, Stalin, Mao?"
00:40:06 - 00:40:08: It actually doesn't take a stance,
00:40:08 - 00:40:10: it says all of them are bad.
00:40:10 - 00:40:12: "These leaders are wildly regarded
00:40:12 - 00:40:15: as some of the worst
dictators in history."
00:40:15 - 00:40:17: Okay, so yeah.
00:40:18 - 00:40:20: Environment.
00:40:22 - 00:40:25: A query for ChatGPT like we just did
00:40:25 - 00:40:30: takes 100 times more energy to execute
00:40:30 - 00:40:31: than a Google search query.
00:40:31 - 00:40:36: Inference, which is producing
the language, takes a lot,
00:40:36 - 00:40:39: is more expensive than
actually training the model.
00:40:39 - 00:40:42: Llama 2 is GPT style model.
00:40:42 - 00:40:43: While they were training it,
00:40:43 - 00:40:47: it produced 539 metric tonnes of CO.
00:40:48 - 00:40:49: The larger the models get,
00:40:49 - 00:40:53: the more energy they need and they emit
00:40:53 - 00:40:54: during their deployment.
00:40:54 - 00:40:57: Imagine lots of them sitting around.
00:40:58 - 00:40:59: Society.
00:41:01 - 00:41:03: Some jobs will be lost.
00:41:03 - 00:41:04: We cannot beat around the bush.
00:41:04 - 00:41:07: I mean, Goldman Sachs
predicted 300 million jobs.
00:41:07 - 00:41:11: I'm not sure this, you know,
we cannot tell the future,
00:41:11 - 00:41:16: but some jobs will be at risk,
like repetitive text writing.
00:41:18 - 00:41:19: Creating fakes.
00:41:20 - 00:41:23: So these are all documented
cases in the news.
00:41:23 - 00:41:26: So a college kid wrote this blog
00:41:26 - 00:41:31: which apparently fooled
everybody using ChatGPT.
00:41:31 - 00:41:34: They can produce fake news.
00:41:34 - 00:41:37: And this is a song, how
many of you know this?
00:41:37 - 00:41:41: So I know I said I'm
gonna be focusing on text
00:41:42 - 00:41:45: but the same technology
you can use in audio,
00:41:45 - 00:41:50: and this is a well-documented
case where somebody, unknown,
00:41:50 - 00:41:55: created this song and it
supposedly was a collaboration
00:41:55 - 00:41:57: between Drake and The Weeknd.
00:41:57 - 00:41:59: Do people know who these are?
00:41:59 - 00:42:01: They are, yeah, very
good, Canadian rappers.
00:42:01 - 00:42:03: And they're not so bad, so.
00:42:06 - 00:42:08: Shall I play the song?
00:42:08 - 00:42:09: - Yeah.
- Okay.
00:42:09 - 00:42:11: Apparently it's very authentic.
00:42:11 - 00:42:14: (bright music)
00:42:17 - 00:42:22:  I came in with my ex
like Selena to flex, ay 
00:42:22 - 00:42:25:  Bumpin' Justin Bieber,
the fever ain't left, ay 
00:42:25 - 00:42:27:  She know what she need 
00:42:27 - 00:42:31: - Apparently it's
totally believable, okay.
00:42:32 - 00:42:35: Have you seen this same
technology but kind of different?
00:42:35 - 00:42:39: This is a deep fake showing
that Trump was arrested.
00:42:39 - 00:42:41: How can you tell it's a deep fake?
00:42:43 - 00:42:46: The hand, yeah, it's too short, right?
00:42:46 - 00:42:50: Yeah, you can see it's like
almost there, not there.
00:42:50 - 00:42:54: Okay, so I have two slides on the future
00:42:54 - 00:42:56: before they come and kick me out
00:42:56 - 00:42:57: because I was told I
have to finish at 8:00
00:42:57 - 00:42:59: to take some questions.
00:42:59 - 00:43:01: Okay, tomorrow.
00:43:01 - 00:43:05: So we can't predict the future
00:43:05 - 00:43:07: and no, I don't think
that these evil computers
00:43:07 - 00:43:10: are gonna come and kill us all.
00:43:10 - 00:43:13: I will leave you with some
thoughts by Tim Berners-Lee.
00:43:13 - 00:43:16: For people who don't know
him, he invented the internet.
00:43:16 - 00:43:19: He's actually Sir Tim Berners-Lee.
00:43:19 - 00:43:22: And he said two things
that made sense to me.
00:43:22 - 00:43:24: First of all, that we don't actually know
00:43:24 - 00:43:27: what a super intelligent
AI would look like.
00:43:27 - 00:43:30: We haven't made it, so it's
hard to make these statements.
00:43:30 - 00:43:35: However, it's likely to have
lots of these intelligent AIs,
00:43:35 - 00:43:38: and by intelligent AIs
we mean things like GPT,
00:43:38 - 00:43:42: and many of them will be good
and will help us do things.
00:43:42 - 00:43:47: Some may fall to the hands of individuals
00:43:49 - 00:43:50: that want to do harm,
00:43:50 - 00:43:54: and it seems easier to minimise the harm
00:43:54 - 00:43:56: that these tools will do
00:43:56 - 00:44:00: than to prevent the systems
from existing at all.
00:44:00 - 00:44:02: So we cannot actually
eliminate them altogether,
00:44:02 - 00:44:05: but we as a society can
actually mitigate the risks.
00:44:06 - 00:44:07: This is very interesting,
00:44:07 - 00:44:10: this is the Australian Research Council
00:44:10 - 00:44:12: that committed a survey
00:44:12 - 00:44:15: and they dealt with a
hypothetical scenario
00:44:15 - 00:44:20: that whether Chad GPT-4
could autonomous replicate,
00:44:21 - 00:44:23: you know, you are replicating yourself,
00:44:23 - 00:44:25: you're creating a copy,
00:44:25 - 00:44:29: acquire resources and
basically be a very bad agent,
00:44:29 - 00:44:30: the things of the movies.
00:44:30 - 00:44:35: And the answer is no, it
cannot do this, it cannot.
00:44:35 - 00:44:37: And they had like some specific tests
00:44:37 - 00:44:39: and it failed on all of them,
00:44:39 - 00:44:41: such as setting up an
open source language model
00:44:41 - 00:44:44: on a new server, it cannot do that.
00:44:45 - 00:44:46: Okay, last slide.
00:44:46 - 00:44:50: So my take on this is that
we cannot turn back time.
00:44:52 - 00:44:57: And every time you think about
AI coming there to kill you,
00:44:57 - 00:44:59: you should think what is the
bigger threat to mankind,
00:44:59 - 00:45:02: AI or climate change?
00:45:02 - 00:45:04: I would personally argue climate
change is gonna wipe us all
00:45:04 - 00:45:07: before the AI becomes super intelligent.
00:45:08 - 00:45:10: Who is in control of AI?
00:45:10 - 00:45:13: There are some humans there
who hopefully have sense.
00:45:13 - 00:45:16: And who benefits from it?
00:45:16 - 00:45:18: Does the benefit outweigh the risk?
00:45:18 - 00:45:21: In some cases, the benefit
does, in others it doesn't.
00:45:21 - 00:45:24: And history tells us
00:45:24 - 00:45:26: that all technology that has been risky,
00:45:26 - 00:45:29: such as, for example, nuclear energy,
00:45:29 - 00:45:32: has been very strongly regulated.
00:45:32 - 00:45:35: So regulation is coming
and watch out the space.
00:45:35 - 00:45:40: And with that I will stop and
actually take your questions.
00:45:40 - 00:45:42: Thank you so much for
listening, you've been great.
00:45:42 - 00:45:45: (audience applauds)
00:45:51 - 00:45:54: (applause fades out)
